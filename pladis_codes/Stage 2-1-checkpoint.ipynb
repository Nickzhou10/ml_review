{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analysis and wrangling\n",
    "import scipy.stats\n",
    "from IPython.core.display import HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "from itertools import chain\n",
    "# visualization\n",
    "from IPython.core.display import HTML\n",
    "from pandas.plotting import scatter_matrix\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from pprint import pprint\n",
    "%matplotlib inline\n",
    "#plotly\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "from plotly.offline import download_plotlyjs,init_notebook_mode, plot, iplot\n",
    "import plotly as py \n",
    "import plotly.graph_objs as go # plotly graphical object\n",
    "import plotly.io as pio\n",
    "pio.renderers.default='notebook'\n",
    "# setting the general visualization style\n",
    "sns.set_style('whitegrid')\n",
    "# feature engineering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Libraries for Statistical Models\n",
    "import statsmodels.api as sm\n",
    "# ignoring warnings in the notebook\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore') \n",
    "# To display full output \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# feature engineering\n",
    "import pandas_profiling as pp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# machine learning models\\\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn import datasets, linear_model\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Lasso,ElasticNet\n",
    "# model selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "\n",
    "# Tuning and Esembling\n",
    "from sklearn.model_selection import GridSearchCV        \n",
    "from sklearn.model_selection import RandomizedSearchCV \n",
    "from mlxtend.regressor import StackingCVRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotly(name):\n",
    "    df = pd.read_csv(\"/project/data_for_models/feature_eng3.csv\",index_col=0)\n",
    "    fig = px.line(df[df['full_name']== name],\n",
    "              x='date', y='price_per_unit', title='Price_per_units for '+ name,\n",
    "              color='channel', template=\"none\")\n",
    "    fig.update_xaxes(\n",
    "        rangeslider_visible=True,\n",
    "        rangeselector=dict(\n",
    "            buttons=list([\n",
    "                dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n",
    "                dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n",
    "                dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n",
    "                dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n",
    "                dict(step=\"all\")\n",
    "            ])\n",
    "        )\n",
    "    )\n",
    "    fig.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the competitors under the same subsegment by using the 2D correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/project/data_for_models/feature_eng3.csv\",index_col=0)\n",
    "df['units'] = df['Sales']/df['price_per_unit']\n",
    "EDT = df.loc[(df.Subsegment == 'EDT' )]\n",
    "EDB = df.loc[(df.Subsegment == 'EDB' )]\n",
    "CBB = df.loc[(df.Subsegment == 'CBB' )]\n",
    "Healthier = df.loc[(df.Subsegment == 'Healthier' )]\n",
    "EDT_pivot = Healthier.pivot_table(index=[\"date\"], columns=['full_name','channel'], \n",
    "                                     values=['units']).reset_index()\n",
    "EDT_corr = EDT_pivot.corr()\n",
    "display(HTML(EDT_corr.to_html()))\n",
    "EDT_corr.to_excel(\"/project/results/correlation_map.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering the top 5 rivals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/project/data_for_models/feature_eng3.csv\",index_col=0)\n",
    "data = []\n",
    "for k in ['EDT','EDB','CBB','Healthier']:\n",
    "    df = pd.read_csv(\"/project/data_for_models/feature_eng3.csv\",index_col=0)\n",
    "    df['units'] = df['Sales']/df['price_per_unit']\n",
    "    for i in df.loc[(df.Subsegment == k ),'full_name'].drop_duplicates().values.tolist():\n",
    "        for j in df.loc[(df.Subsegment == k)& (df.full_name == i),'channel'].drop_duplicates().values.tolist():\n",
    "            df = pd.read_csv(\"/project/data_for_models/feature_eng3.csv\",index_col=0)\n",
    "            df['units'] = df['Sales']/df['price_per_unit']\n",
    "            EDT = df.loc[(df.Subsegment == k )]\n",
    "            EDT = EDT.pivot_table(index=[\"date\"], columns=['full_name','channel'], \n",
    "                                                  values=['units']).reset_index()\n",
    "            EDT_corr = EDT.corr()\n",
    "            EDT_corr = EDT_corr.loc[('units',i,j)][EDT_corr.loc[('units',i,j)]<-0.2].sort_values()\n",
    "            EDT_corr = EDT_corr.reset_index()\n",
    "            EDT_corr = EDT_corr.drop(['level_0'], axis=1)\n",
    "            EDT_corr = EDT_corr[0:5]\n",
    "            EDT_corr['sku'] = i\n",
    "            EDT_corr['sku_channel'] = j\n",
    "            EDT_corr['subsegment'] = k\n",
    "            EDT_corr.columns = ['rivals', 'rival_channel','units_correlation','sku','sku_channel','subsegment']\n",
    "            data.append(EDT_corr)\n",
    "df_corr = pd.concat(data)\n",
    "df_corr.loc[df_corr.units_correlation<-0.2,'Competition_level'] = 'Low'\n",
    "df_corr.loc[df_corr.units_correlation<-0.4,'Competition_level'] = 'Medium'\n",
    "df_corr.loc[df_corr.units_correlation<-0.6,'Competition_level'] = 'High'\n",
    "df_corr['rival_rank'] = df_corr.index\n",
    "df['Avg_sub_share'] = df['Sales']/df['sub_index']\n",
    "sub_share = df.groupby(['full_name','channel'], as_index=False)['Avg_sub_share'].mean()\n",
    "sub_share.rename(columns={'full_name':'sku','channel':'sku_channel'}, inplace=True)\n",
    "sub_share = sub_share.round(4)\n",
    "df_corr = pd.merge(df_corr, sub_share, on=['sku','sku_channel'], how='left')\n",
    "sub_share.rename(columns={'Avg_sub_share':'Avg_rival_share','sku':'rivals','sku_channel':'rival_channel'}, inplace=True)\n",
    "df_corr = pd.merge(df_corr, sub_share, on=['rivals','rival_channel'], how='left')\n",
    "\n",
    "basep = df[['full_name','channel','base_price1']].groupby(\n",
    "    ['full_name','channel'], as_index=False)['base_price1'].mean()\n",
    "basep.rename(columns={'full_name':'sku','channel':'sku_channel','base_price1':'Avg_base_p'}, inplace=True)\n",
    "df_corr = pd.merge(df_corr,basep, on=['sku','sku_channel'], how='left')\n",
    "basep.rename(columns={'sku':'rivals','sku_channel':'rival_channel','Avg_base_p':'Avg_rival_base'}, inplace=True)\n",
    "df_corr = pd.merge(df_corr,basep, on=['rivals','rival_channel'], how='left')\n",
    "\n",
    "depth = df[['full_name','channel','Promo_depth']]\n",
    "depth = depth[depth['Promo_depth']!=0]\n",
    "depth = depth.groupby(['full_name','channel'], as_index=False)['Promo_depth'].mean()\n",
    "depth.rename(columns={'full_name':'sku','channel':'sku_channel','Promo_depth':'Avg_promo_dep'}, inplace=True)\n",
    "df_corr = pd.merge(df_corr,depth, on=['sku','sku_channel'], how='left')\n",
    "depth.rename(columns={'sku':'rivals','sku_channel':'rival_channel','Avg_promo_dep':'Avg_rival_dep'}, inplace=True)\n",
    "df_corr = pd.merge(df_corr,depth, on=['rivals','rival_channel'], how='left')\n",
    "df_corr = df_corr.fillna(0)\n",
    "df_corr = df_corr[['rival_rank','subsegment','sku','sku_channel','Avg_base_p','Avg_promo_dep','Avg_sub_share',\n",
    "                   'units_correlation','Competition_level','rivals', 'rival_channel','Avg_rival_base','Avg_rival_dep','Avg_rival_share']]\n",
    "df_corr.rename(columns={'units_correlation':'competition_score'}, inplace=True)\n",
    "df_corr['competition_score'] = abs(df_corr['competition_score'])\n",
    "df_corr.to_csv(\"/project/data_for_models/units_correlation.csv\")\n",
    "display(HTML(df_corr.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for k in ['EDT','EDB','CBB','Healthier']:\n",
    "    df = pd.read_csv(\"/project/data_for_models/feature_eng3.csv\",index_col=0)\n",
    "    df['units'] = df['Sales']/df['price_per_unit']\n",
    "    for i in df.loc[(df.Subsegment == k ),'full_name'].drop_duplicates().values.tolist():\n",
    "        for j in df.loc[(df.Subsegment == k)& (df.full_name == i),'channel'].drop_duplicates().values.tolist():\n",
    "            df = pd.read_csv(\"/project/data_for_models/feature_eng3.csv\",index_col=0)\n",
    "            df['units'] = df['Sales']/df['price_per_unit']\n",
    "            EDT = df.loc[(df.Subsegment == k )]\n",
    "            EDT = EDT.pivot_table(index=[\"date\"], columns=['full_name','channel'], \n",
    "                                                  values=['units']).reset_index()\n",
    "            EDT_corr = EDT.corr()\n",
    "            EDT_corr = EDT_corr.loc[('units',i,j)][EDT_corr.loc[('units',i,j)]<-0.2].sort_values()\n",
    "            EDT_corr = EDT_corr.reset_index()\n",
    "            EDT_corr = EDT_corr.drop(['level_0'], axis=1)\n",
    "            EDT_corr = EDT_corr[0:8]\n",
    "            EDT_corr['sku'] = i\n",
    "            EDT_corr['sku_channel'] = j\n",
    "            EDT_corr['subsegment'] = k\n",
    "            EDT_corr.columns = ['rivals', 'rival_channel','units_correlation','sku','sku_channel','subsegment']\n",
    "            data.append(EDT_corr)\n",
    "df_corr = pd.concat(data)\n",
    "df_corr.loc[df_corr.units_correlation<-0.2,'Competition_level'] = 'Low'\n",
    "df_corr.loc[df_corr.units_correlation<-0.4,'Competition_level'] = 'Medium'\n",
    "df_corr.loc[df_corr.units_correlation<-0.6,'Competition_level'] = 'High'\n",
    "df_corr = df_corr[['sku','sku_channel','subsegment','units_correlation','Competition_level','rivals', 'rival_channel']]\n",
    "df['Avg_sub_share'] = df['Sales']/df['sub_index']\n",
    "sub_share = df.groupby(['full_name','channel'], as_index=False)['Avg_sub_share'].mean()\n",
    "sub_share.rename(columns={'full_name':'sku','channel':'sku_channel'}, inplace=True)\n",
    "sub_share = sub_share.round(4)\n",
    "\n",
    "df_corr = pd.merge(df_corr, sub_share, on=['sku','sku_channel'], how='left')\n",
    "sub_share.rename(columns={'Avg_sub_share':'Avg_rival_share','sku':'rivals','sku_channel':'rival_channel'}, inplace=True)\n",
    "df_corr = pd.merge(df_corr, sub_share, on=['rivals','rival_channel'], how='left')\n",
    "df_corr = df_corr[['subsegment','sku','sku_channel','Avg_sub_share','units_correlation','Competition_level','rivals', 'rival_channel','Avg_rival_share']]\n",
    "\n",
    "competition = df_corr.groupby(['subsegment','sku_channel'], as_index=False)['units_correlation'].mean()\n",
    "competition['competition_score'] = abs(competition['units_correlation'])\n",
    "competition = competition.sort_values(by=['competition_score','sku_channel'],ascending=False)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(20, 10))\n",
    "ax = sns.barplot(x=\"subsegment\", y=\"competition_score\",hue = 'sku_channel' ,data=competition,\n",
    "                 palette=\"OrRd_r\")\n",
    "\n",
    "plt.title(\"Avg. level of competition by subsegments and channels\", fontsize =20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for k in ['EDT','EDB','CBB','Healthier']:\n",
    "    df = pd.read_csv(\"/project/data_for_models/feature_eng3.csv\",index_col=0)\n",
    "    df['units'] = df['Sales']/df['price_per_unit']\n",
    "    for i in df.loc[(df.Subsegment == k ),'full_name'].drop_duplicates().values.tolist():\n",
    "        for j in df.loc[(df.Subsegment == k)& (df.full_name == i),'channel'].drop_duplicates().values.tolist():\n",
    "            df = pd.read_csv(\"/project/data_for_models/feature_eng3.csv\",index_col=0)\n",
    "            df['units'] = df['Sales']/df['price_per_unit']\n",
    "            EDT = df.loc[(df.Subsegment == k )]\n",
    "            EDT = EDT.pivot_table(index=[\"date\"], columns=['full_name','channel'], \n",
    "                                                  values=['units']).reset_index()\n",
    "            EDT_corr = EDT.corr()\n",
    "            EDT_corr = EDT_corr.loc[('units',i,j)][EDT_corr.loc[('units',i,j)]<-0.1].sort_values()\n",
    "            EDT_corr = EDT_corr.reset_index()\n",
    "            EDT_corr = EDT_corr.drop(['level_0'], axis=1)\n",
    "            EDT_corr = EDT_corr[0:5]\n",
    "            EDT_corr['sku'] = i\n",
    "            EDT_corr['sku_channel'] = j\n",
    "            EDT_corr['subsegment'] = k\n",
    "            EDT_corr.columns = ['rivals', 'rival_channel','units_correlation','sku','sku_channel','subsegment']\n",
    "            data.append(EDT_corr)\n",
    "df_corr = pd.concat(data)\n",
    "df_corr.loc[df_corr.units_correlation<-0.2,'Competition_level'] = 'Low'\n",
    "df_corr.loc[df_corr.units_correlation<-0.4,'Competition_level'] = 'Medium'\n",
    "df_corr.loc[df_corr.units_correlation<-0.6,'Competition_level'] = 'High'\n",
    "df_corr = df_corr[['sku','sku_channel','subsegment','units_correlation','Competition_level','rivals', 'rival_channel']]\n",
    "df['Avg_sub_share'] = df['Sales']/df['sub_index']\n",
    "sub_share = df.groupby(['full_name','channel'], as_index=False)['Avg_sub_share'].mean()\n",
    "sub_share.rename(columns={'full_name':'sku','channel':'sku_channel'}, inplace=True)\n",
    "sub_share = sub_share.round(4)\n",
    "\n",
    "df_corr = pd.merge(df_corr, sub_share, on=['sku','sku_channel'], how='left')\n",
    "sub_share.rename(columns={'Avg_sub_share':'Avg_rival_share','sku':'rivals','sku_channel':'rival_channel'}, inplace=True)\n",
    "df_corr = pd.merge(df_corr, sub_share, on=['rivals','rival_channel'], how='left')\n",
    "df_corr = df_corr[['subsegment','sku','sku_channel','Avg_sub_share','units_correlation','Competition_level','rivals', 'rival_channel','Avg_rival_share']]\n",
    "\n",
    "plotly = df_corr.groupby(['sku','sku_channel','Avg_sub_share','subsegment'], as_index=False)['units_correlation','Avg_rival_share'].mean()\n",
    "plotly['competition_score'] = abs(plotly['units_correlation'])\n",
    "fig = px.scatter_3d(plotly, x='Avg_rival_share', y='Avg_sub_share', z='competition_score',\n",
    "              color='subsegment', title='3D scatter plot for Avg.sub share, Avg.rival share, and competition score',\n",
    "                   range_x = [0,0.1])\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=670,)\n",
    "fig.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>  Insights from the 3D chart:</b> \n",
    "   \n",
    "- Competition tend to happens between SKUs with similar market shares under the same channel.\n",
    "- The competition level increases as the market shares for both parties increase.\n",
    "- However, when the share of SKUs reaches around 3-4%, the competition level and the share of rivals starts to diminish. The phenomenon is very clear for EDT and CBB.\n",
    "- Thus, SKUs with more market share will be less affected by competition and take advantages of more monopolistic power.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = pd.read_csv(\"/project/data_for_models/units_correlation.csv\")\n",
    "plotly2 = df_corr.groupby(['sku','sku_channel','Avg_base_p','subsegment'], as_index=False)['competition_score','Avg_rival_base'].mean()\n",
    "fig = px.scatter_3d(plotly2, x='Avg_base_p', y='Avg_rival_base', z='competition_score',\n",
    "              color='subsegment', title='3D scatter plot for Avg.sub base price, Avg.rival base price, and competition score',\n",
    "                  )\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=670,)\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>  Insights from the 3D chart:</b> \n",
    "   \n",
    "- competitions are centered around base prices of 1.3 to 1.8.\n",
    "- sebsegments can be classified by different base price intervals.\n",
    "- Competition tend to be higher for SKUs with similar base prices under the same subsegment.\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is such competition caused by rivals' promo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/project/data_for_models/feature_eng3.csv\",index_col=0)\n",
    "df_corr = pd.read_csv(\"/project/data_for_models/units_correlation.csv\",index_col=0)\n",
    "df['units'] = df['Sales']/df['price_per_unit']\n",
    "promo_corr = []\n",
    "for i in df_corr['sku'].drop_duplicates().values.tolist():\n",
    "    for j in df_corr.loc[df_corr.sku == i,'sku_channel'].drop_duplicates().values.tolist():\n",
    "        for k in df_corr.loc[(df_corr.sku == i)&(df_corr.sku_channel == j),'rivals'].drop_duplicates().values.tolist():\n",
    "            for l in df_corr.loc[(df_corr.sku == i)&(df_corr.sku_channel == j)&(df_corr.rivals == k),'rival_channel'].drop_duplicates().values.tolist():\n",
    "                X = df.loc[(df.full_name == i)&(df.channel == j),['units','date']]\n",
    "                y = df.loc[(df.full_name == k)&(df.channel == l),['Promo_depth','date']]\n",
    "                xy = pd.merge(X, y, on=['date'], how='left')\n",
    "                corr_matrix = xy.corr()\n",
    "                promo_corr.append(corr_matrix.iat[0, 1])\n",
    "df_corr['promo_corr'] = promo_corr\n",
    "df_corr['promo_corr'] = df_corr['promo_corr'].fillna(0)\n",
    "df_corr.rename(columns={'promo_corr':'promo_score'}, inplace=True)\n",
    "df_corr['promo_score'] = abs(df_corr['promo_score'])\n",
    "df_corr = df_corr[['rival_rank','subsegment','sku','sku_channel','Avg_sub_share','Avg_sub_share','Avg_base_p','Avg_promo_dep',\n",
    "                   'competition_score','promo_score','Competition_level','rivals', 'rival_channel','Avg_rival_share','Avg_rival_base','Avg_rival_dep']]\n",
    "df_corr.to_csv(\"/project/data_for_models/promo_corr.csv\")\n",
    "display(HTML(df_corr.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By changing in base prices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/project/data_for_models/feature_eng3.csv\",index_col=0)\n",
    "df_corr = pd.read_csv(\"/project/data_for_models/promo_corr.csv\",index_col=0)\n",
    "df['units'] = df['Sales']/df['price_per_unit']\n",
    "base_corr = []\n",
    "for i in df_corr['sku'].drop_duplicates().values.tolist():\n",
    "    for j in df_corr.loc[df_corr.sku == i,'sku_channel'].drop_duplicates().values.tolist():\n",
    "        for k in df_corr.loc[(df_corr.sku == i)&(df_corr.sku_channel == j),'rivals'].drop_duplicates().values.tolist():\n",
    "            for l in df_corr.loc[(df_corr.sku == i)&(df_corr.sku_channel == j)&(df_corr.rivals == k),'rival_channel'].drop_duplicates().values.tolist():\n",
    "                X = df.loc[(df.full_name == i)&(df.channel == j),['units','date']]\n",
    "                y = df.loc[(df.full_name == k)&(df.channel == l),['base_price1','date']]\n",
    "                xy = pd.merge(X, y, on=['date'], how='left')\n",
    "                corr_matrix = xy.corr()\n",
    "                base_corr.append(corr_matrix.iat[0, 1])\n",
    "df_corr['base_corr'] = base_corr\n",
    "df_corr['base_corr'] = df_corr['base_corr'].fillna(0)\n",
    "df_corr.rename(columns={'base_corr':'base_score'}, inplace=True)\n",
    "df_corr['base_score'] = abs(df_corr['base_score'])\n",
    "df_corr = df_corr[['rival_rank','subsegment','sku','sku_channel','Avg_sub_share','Avg_base_p','Avg_sub_share','Avg_promo_dep',\n",
    "                   'competition_score','promo_score','base_score','Competition_level','rivals', 'rival_channel','Avg_rival_share','Avg_rival_base','Avg_rival_dep']]\n",
    "display(HTML(df_corr.to_html()))\n",
    "df_corr.to_csv(\"/project/data_for_models/base_corr.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantifying the competition by Cross-Price Elasticities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/project/data_for_models/feature_eng3.csv\",index_col=0)\n",
    "df_corr = pd.read_csv(\"/project/data_for_models/base_corr.csv\")\n",
    "df['units'] = df['Sales']/df['price_per_unit']\n",
    "coe = []\n",
    "for i in df_corr['sku'].drop_duplicates().values.tolist():\n",
    "    for j in df_corr.loc[df_corr.sku == i,'sku_channel'].drop_duplicates().values.tolist():\n",
    "        for k in df_corr.loc[(df_corr.sku == i)&(df_corr.sku_channel == j),'rivals'].drop_duplicates().values.tolist():\n",
    "            for l in df_corr.loc[(df_corr.sku == i)&(df_corr.sku_channel == j)&(df_corr.rivals == k),'rival_channel'].drop_duplicates().values.tolist():\n",
    "                y = df.loc[(df.full_name == i)&(df.channel == j),['units','date']]\n",
    "                X = df.loc[(df.full_name == k)&(df.channel == l),['price_per_unit','date']]\n",
    "                data1 = pd.merge(y, X, on=['date'], how='left')  \n",
    "                data1 = data1.dropna()\n",
    "                y = np.log(data1['units'])\n",
    "                data1 = data1.drop(['units','date'], axis=1)\n",
    "                X = np.log(data1)\n",
    "                lm = linear_model.LinearRegression()\n",
    "                lm.fit(X,y)\n",
    "                coe.append(lm.coef_[0])\n",
    "df_corr['Cross_coefficient'] = coe\n",
    "#df_corr = df_corr.drop(['Unnamed: 0'], axis=1)\n",
    "df_corr.to_csv(\"/project/data_for_models/coe_corr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_corr = df_corr.drop(['Unnamed: 0'], axis=1)\n",
    "df_corr = df_corr[['rival_rank','subsegment','sku','sku_channel','Avg_sub_share','Avg_base_p','Avg_promo_dep','competition_score','promo_score'\n",
    "                   ,'Cross_coefficient','Competition_level','rivals', 'rival_channel','Avg_rival_share','Avg_rival_base','Avg_rival_dep']]\n",
    "df_corr.to_csv(\"/project/data_for_models/final_raw.csv\")\n",
    "\n",
    "display(HTML(df_corr.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>  Interpretation of the Cross price coefficient:</b> \n",
    "   \n",
    "- Keeping everything else unchanged, one percent decrease in one rival's price/unit will decrease the quantity(unit) sold by 1.95%, if the Cross_coefficient is 1.95.\n",
    "- One disadvantage of the Cross price coefficient is that mixed effects from rivals cannot be interpreted.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">    \n",
    "<b> Be careful about the prisoner's dilemma:</b>  \n",
    "   \n",
    "- Inspired by the prisoner's dilemma from the Game theories: \n",
    "If all Retailers treat every item as a key item and price and promo it low to keep up with competitors, an “race to the bottom” will bring unprofitable competitions as each competitor notches down its price to stay below the competition. \n",
    "- Thus, a dynamic and segmented approach to item-level pricing will allow retailers to optimize across multiple objectives (for example, margin, price perception, and market share).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_corr = pd.read_csv(\"/project/data_for_models/final_raw.csv\")\n",
    "df = pd.read_csv(\"/project/data_for_models/feature_eng3.csv\",index_col=0)\n",
    "df = df[['full_name','channel','brand','company','weight','pack_type','flavour']]\n",
    "df.rename(columns={'full_name':'sku','channel':'sku_channel'}, inplace=True)\n",
    "df_corr = pd.merge(df_corr,df, on=['sku','sku_channel'], how='left')\n",
    "df = df[['sku','sku_channel','brand','company','weight','pack_type','flavour']]\n",
    "df.rename(columns={'sku':'rivals','sku_channel':'rival_channel','brand':'rival_brand',\n",
    "                        'weight':'rival_weight','pack_type':'rival_pack','flavour':'rival_flavour','company':'drop'}, inplace=True)\n",
    "df_rank = pd.merge(df_corr,df, on=['rivals','rival_channel'], how='left')\n",
    "df_rank = df_rank.drop(['Unnamed: 0','drop'], axis=1)\n",
    "df_rank = df_rank.drop_duplicates().reset_index(drop = True)\n",
    "df_rank = df_rank[['rival_rank','subsegment','company','brand','sku','sku_channel','weight','pack_type','flavour','Avg_sub_share','Avg_base_p','Avg_promo_dep','competition_score','promo_score'\n",
    "                   ,'Cross_coefficient','Competition_level','rivals','rival_brand', 'rival_channel','rival_weight','rival_pack','rival_flavour','Avg_rival_share','Avg_rival_base','Avg_rival_dep']]\n",
    "\n",
    "df_rank.to_excel(\"/project/results/competition_raw.xlsx\")\n",
    "df_rank = df_rank.loc[(df_rank.company == 'PLADIS UK')&(df_rank.Competition_level != 'Low')].reset_index(drop = True)\n",
    "df_rank = df_rank.drop(['company','weight','pack_type','flavour','Avg_sub_share','rival_weight','rival_pack','rival_flavour','Avg_rival_share'], axis=1)\n",
    "df_rank.to_csv(\"/project/results/elasticnets.csv\")\n",
    "display(HTML(df_rank.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cannibalization refers to when a consumer\n",
    "chooses a promoted product over a similar product they would have otherwise\n",
    "bought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y1 = 'MCVTS CHCLT DGSTVS MLK CHCLT 266 GM SNGL'\n",
    "y2 = 'Tesco Express'\n",
    "x1 = 'JFF CKS DRK CHCLT & ORNG 244 GM SNGL'\n",
    "x2 = 'Tesco Express'\n",
    "df = pd.read_csv(\"/project/data_for_models/feature_eng3.csv\",index_col=0)\n",
    "df['units'] = df['Sales']/df['price_per_unit']\n",
    "df_x = df.loc[(df.full_name == x1)&(df.channel == x2)]\n",
    "df_y = df.loc[(df.full_name == y1)&(df.channel == y2)]\n",
    "df_y = df_y[['date','units','Promo_depth','base_price1','distribution']]\n",
    "df_x = df_x[['date','units','Promo_depth','base_price1','distribution']]\n",
    "df_x.rename(columns={'units':'units_canni','Promo_depth':'Promo_depth_canni',\n",
    "                     'base_price1':'base_price1_canni','distribution':'distribution_canni'}, inplace=True)\n",
    "\n",
    "df_model = pd.merge(df_x,df_y, on=['date'], how='left')\n",
    "y = np.log(df_model['units'])\n",
    "X = df_model.drop(['date','units'], axis=1)\n",
    "X = X[['Promo_depth','base_price1','distribution','Promo_depth_canni','base_price1_canni','distribution_canni']]\n",
    "for i in ['base_price1','base_price1_canni']:\n",
    "    X[i] = np.log(X[i])\n",
    "# define model evaluation method\n",
    "cv = KFold(n_splits = 2,shuffle = True,random_state=42)\n",
    "# define search\n",
    "ratios = np.arange(0, 1, 0.01)\n",
    "alphas = np.arange(0, 1, 0.0001)\n",
    "#alphas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0]\n",
    "model = ElasticNetCV(l1_ratio=ratios, alphas=alphas, cv=cv, n_jobs=-1, random_state=42)\n",
    "# fit model\n",
    "model = model.fit(X, y)\n",
    "regr = ElasticNet(alpha = model.alpha_, l1_ratio = model.l1_ratio_, random_state=42)\n",
    "regr.fit(X, y)\n",
    "reg_result = pd.DataFrame({'Items' :X.columns,'coefficients' :regr.coef_})\n",
    "for i,j in ['Adjusted R2',adjustedR2(X,y, regr)],['Intercept',regr.intercept_]:\n",
    "    reg_result.loc[-1] = [i,j]  # adding a row\n",
    "    reg_result.index = reg_result.index + 1  # shifting index\n",
    "    reg_result = reg_result.sort_index()\n",
    "reg_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjustedR2(x,y, reg):\n",
    "    r2 = reg.score(x,y)\n",
    "    n = x.shape[0]\n",
    "    p = x.shape[1]\n",
    "    adjusted_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "    return adjusted_r2\n",
    "\n",
    "def elastic_nets(y1,y2):\n",
    "    df = pd.read_csv(\"/project/data_for_models/feature_eng3.csv\",index_col=0)\n",
    "    df['units'] = df['Sales']/df['price_per_unit']\n",
    "    df_corr = pd.read_csv(\"/project/results/elasticnets.csv\",index_col = 0)\n",
    "    x = df_corr.loc[(df_corr.sku == y1)&\n",
    "                    (df_corr.sku_channel == y2),['rivals','rival_channel']].reset_index(drop=True)[:10]\n",
    "    df_y = df.loc[(df.full_name == y1)&(df.channel == y2)][['date','units','Promo_depth','base_price1','distribution','sub_index']]\n",
    "    data = []\n",
    "    for i in x['rivals'].drop_duplicates().values.tolist():\n",
    "        for j in x.loc[x.rivals == i,'rival_channel'].drop_duplicates().values.tolist():\n",
    "            data.append(df.loc[(df.full_name == i)&(df.channel == j),['date','units','Promo_depth','base_price1','distribution']])\n",
    "\n",
    "    for num in list(range(0, len(data))):\n",
    "        data[num].columns = [str(col) + '_rival_'+ str(num) for col in data[num].columns]\n",
    "        data[num].rename(columns={data[num].columns[0]: 'date'}, inplace=True)\n",
    "    num = 0\n",
    "    while num < len(data):\n",
    "        df_y = pd.merge(df_y, data[num], on=['date'], how='left')    \n",
    "        num += 1\n",
    "    df_y.dropna(inplace = True)\n",
    "    y = np.log(df_y['units'])\n",
    "    X = df_y.drop(['date','units'], axis=1)\n",
    "    #X = X[['Promo_depth','base_price1','distribution','Promo_depth_canni','base_price1_canni','distribution_canni']]\n",
    "    for i in ['base_price1','distribution']:\n",
    "        for j in X.filter(regex=i).columns:\n",
    "            X[j] = np.log(X[j])\n",
    "    for n in X.filter(regex='Promo_depth').columns:\n",
    "        X[n] = np.log1p(X[n])\n",
    "    # define model evaluation method\n",
    "    cv = KFold(n_splits = 3,shuffle = True,random_state=42)\n",
    "    # define search\n",
    "    grid = dict()\n",
    "    #grid['alpha'] = np.arange(0, 1, 0.00001)\n",
    "    grid['alpha'] = np.arange(0, 1, 0.00001)\n",
    "    grid['l1_ratio'] = np.arange(0, 1, 0.0001)\n",
    "    # define search  \n",
    "    model = ElasticNet(random_state=42)\n",
    "    search = RandomizedSearchCV(model, grid, scoring='neg_root_mean_squared_error',\n",
    "                                verbose=0,n_iter = 10000, cv=cv, random_state=42, n_jobs=-1)\n",
    "    # fit model\n",
    "    model = search.fit(X, y)\n",
    "    regr = ElasticNet(**model.best_params_, random_state=42)\n",
    "    regr.fit(X, y)\n",
    "    reg_result = pd.DataFrame({'Items' :X.columns,'coefficients' :regr.coef_})\n",
    "    for i,j in ['Intercept',np.round(regr.intercept_,4)],['Adjusted R2',np.round(adjustedR2(X,y, regr),4)]:\n",
    "        reg_result.loc[-1] = [i,j]  \n",
    "        reg_result.index = reg_result.index + 1  \n",
    "        reg_result = reg_result.sort_index()\n",
    "    reg_result['coefficients'] = reg_result['coefficients'].round(4)\n",
    "    reg_result.set_index('Items', inplace = True)\n",
    "    reg_result.rename(columns={'coefficients':y2})\n",
    "    reg_result.columns = pd.MultiIndex.from_product([[y1],[y2]])\n",
    "    return reg_result\n",
    "\n",
    "df_corr = pd.read_csv(\"/project/results/elasticnets.csv\",index_col = 0)\n",
    "final = []\n",
    "for i in df_corr['sku'].drop_duplicates().values.tolist():\n",
    "    for j in df_corr.loc[df_corr.sku == i,'sku_channel'].drop_duplicates().values.tolist():\n",
    "        results = elastic_nets(i,j)\n",
    "        final.append(results)\n",
    "num = 1\n",
    "final_table = final[0]\n",
    "while num < len(final):\n",
    "    final_table = pd.concat([final_table, final[num]], axis=1) \n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(HTML(final_table.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(HTML(df_corr.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "two-way causal relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1,df2 = df_corr,final_table\n",
    "with pd.ExcelWriter('/project/results/output.xlsx') as writer:  \n",
    "    df1.to_excel(writer, sheet_name='Dictionary of competition')\n",
    "    df2.to_excel(writer, sheet_name='Modeling results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/project/data_for_models/feature_eng3.csv\",index_col=0)\n",
    "df['units'] = df['Sales']/df['price_per_unit']\n",
    "df_corr = pd.read_csv(\"/project/results/elasticnets.csv\",index_col = 0)\n",
    "df_corr = df_corr[['sku','sku_channel']].drop_duplicates()\n",
    "df_reg = df.loc[df.company == 'PLADIS UK',['full_name','channel']].drop_duplicates()\n",
    "df_reg['merged'] = df_reg[['full_name','channel']].apply(lambda x: ', '.join(x[x.notnull()]), axis = 1)\n",
    "df_corr['merged'] = df_corr[['sku','sku_channel']].apply(lambda x: ', '.join(x[x.notnull()]), axis = 1)\n",
    "df_reg_1 = df_reg[~df_reg.merged.isin(df_corr.merged)].drop(['merged'], axis=1)\n",
    "df_reg_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_values(i,j):\n",
    "    data = df.loc[(df.full_name == i)&(df.channel == j),\n",
    "                  ['date','units','Promo_depth','base_price1','distribution','sub_index']]\n",
    "    y = np.log(data['units'])\n",
    "    #y = data['units']\n",
    "    X = data.drop(['date','units'], axis=1)\n",
    "    for n in ['base_price1','distribution','sub_index']:\n",
    "        X[n] = np.log(X[n])\n",
    "    model = SelectKBest(score_func=f_regression,k='all').fit(X,y)\n",
    "    reg_result = pd.DataFrame({'Items' :X.columns,'coefficients' :model.pvalues_})\n",
    "    reg_result.set_index('Items', inplace = True)\n",
    "    reg_result.rename(columns={'P values':j})\n",
    "    reg_result.columns = pd.MultiIndex.from_product([[i],[j]])\n",
    "    return reg_result\n",
    "\n",
    "def regression(i,j):\n",
    "    data = df.loc[(df.full_name == i)&(df.channel == j),\n",
    "                  ['date','units','Promo_depth','base_price1','distribution','sub_index']]\n",
    "    y = np.log(data['units'])\n",
    "    #y = data['units']\n",
    "    X = data.drop(['date','units'], axis=1)\n",
    "    for n in ['base_price1','distribution','sub_index']:\n",
    "        X[n] = np.log(X[n])\n",
    "    X['Promo_depth'] = np.log1p(X['Promo_depth']) \n",
    "    reg = linear_model.LinearRegression()\n",
    "    reg.fit(X,y)\n",
    "    reg_result = pd.DataFrame({'Items' :X.columns,'coefficients' :reg.coef_})\n",
    "    for a,b in ['Intercept',np.round(reg.intercept_,4)],['Adjusted R2',np.round(adjustedR2(X,y, reg),4)]:\n",
    "        reg_result.loc[-1] = [a,b]  # adding a row\n",
    "        reg_result.index = reg_result.index + 1  # shifting index\n",
    "        reg_result = reg_result.sort_index()\n",
    "    reg_result['coefficients'] = reg_result['coefficients'].round(4)\n",
    "    reg_result.set_index('Items', inplace = True)\n",
    "    reg_result.rename(columns={'coefficients':j})\n",
    "    reg_result.columns = pd.MultiIndex.from_product([[i],[j]])\n",
    "    return reg_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/project/data_for_models/feature_eng3.csv\",index_col=0)\n",
    "df['units'] = df['Sales']/df['price_per_unit']\n",
    "df_corr = pd.read_csv(\"/project/results/elasticnets.csv\",index_col = 0)\n",
    "df_corr = df_corr[['sku','sku_channel']].drop_duplicates()\n",
    "df_reg = df.loc[df.company == 'PLADIS UK',['full_name','channel']].drop_duplicates()\n",
    "final = []\n",
    "for i in df_reg_1['full_name'].drop_duplicates().values.tolist():\n",
    "    for j in df_reg_1.loc[df_reg_1.full_name == i,'channel'].drop_duplicates().values.tolist():\n",
    "        results = p_values(i,j)\n",
    "        final.append(results)\n",
    "num = 1\n",
    "p_table = final[0]\n",
    "while num < len(final):\n",
    "    p_table = pd.concat([p_table, final[num]], axis=1) \n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/project/data_for_models/feature_eng3.csv\",index_col=0)\n",
    "df['units'] = df['Sales']/df['price_per_unit']\n",
    "df_corr = pd.read_csv(\"/project/results/elasticnets.csv\",index_col = 0)\n",
    "df_corr = df_corr[['sku','sku_channel']].drop_duplicates()\n",
    "df_reg = df.loc[df.company == 'PLADIS UK',['full_name','channel']].drop_duplicates()\n",
    "df_reg_1 = df_reg\n",
    "final = []\n",
    "for i in df_reg_1['full_name'].drop_duplicates().values.tolist():\n",
    "    for j in df_reg_1.loc[df_reg_1.full_name == i,'channel'].drop_duplicates().values.tolist():\n",
    "        results = regression(i,j)\n",
    "        final.append(results)\n",
    "num = 1\n",
    "final_table = final[0]\n",
    "while num < len(final):\n",
    "    final_table = pd.concat([final_table, final[num]], axis=1) \n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_table.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid = final_table[p_table<0.05]\n",
    "valid[0:1],valid[1:2] = final_table[0:1],final_table[1:2]\n",
    "valid[valid.isnull()] = 'Insignificant'\n",
    "final_table = valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p_table1 = p_table.append(final_table[:2])\n",
    "\n",
    "#p_table1 = p_table1.reindex([\"Adjusted R2\", \"Intercept\", \"Promo_depth\",'base_price1','distribution','sub_index'])\n",
    "#def highlight(value):\n",
    "#    bool_matrix = p_table1>=0.05\n",
    "#    bool_matrix.iloc[:2] = False\n",
    "#    return bool_matrix.applymap(lambda x: 'background-color: yellow' if x else '')\n",
    "#final_table = final_table.style.apply(highlight, axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presumably, you included a variable in your model because you thought (from past experience or expert opinion) that it played an important part in a customer deciding if they will buy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_table_t,final_table_t = p_table.T,final_table.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1,df2 = p_table_t,final_table_t\n",
    "with pd.ExcelWriter('/project/results/output_OLS_insig.xlsx') as writer:  \n",
    "    df1.to_excel(writer, sheet_name='p_table')\n",
    "    df2.to_excel(writer, sheet_name='OLS results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_table,p_table = final_table_t,p_table_t\n",
    "final_table.Promo_depth = final_table.Promo_depth.astype('str', copy=False)\n",
    "final_table.Promo_depth = final_table.Promo_depth.astype('str', copy=False)\n",
    "final_table.base_price1 = final_table.base_price1.astype('str', copy=False)\n",
    "final_table.sub_index = final_table.sub_index.astype('str', copy=False)\n",
    "final_table.loc[final_table.Promo_depth>'0','Effectiveness of Promo depth'] = 'Very Low'\n",
    "final_table.loc[final_table.Promo_depth>'2','Effectiveness of Promo depth'] = 'Low'\n",
    "final_table.loc[final_table.Promo_depth>'3','Effectiveness of Promo depth'] = 'Mid'\n",
    "final_table.loc[final_table.Promo_depth>'4','Effectiveness of Promo depth'] = 'High'\n",
    "final_table.loc[final_table.Promo_depth>'5','Effectiveness of Promo depth'] = 'Very High'\n",
    "final_table.loc[final_table.Promo_depth=='Insignificant','Effectiveness of Promo depth'] = 'Unclear/very low'\n",
    "\n",
    "\n",
    "final_table.loc[final_table.base_price1<'-100','Negative impact of Base Price increase'] = 'High'\n",
    "final_table.loc[final_table.base_price1<'-1','Negative impact of Base Price increase'] = 'Mid'\n",
    "final_table.loc[final_table.base_price1<'-0.5','Negative impact of Base Price increase'] = 'Low'\n",
    "final_table.loc[final_table.base_price1>'0','Negative impact of Base Price increase'] = 'to be checked'\n",
    "final_table.loc[final_table.base_price1=='Insignificant','Negative impact of Base Price increase'] = 'Unclear/very low'\n",
    "\n",
    "final_table.loc[final_table.sub_index>'0','Responsiveness to systematic changes'] = 'Very Low'\n",
    "final_table.loc[final_table.sub_index>'0.7','Responsiveness to systematic changes'] = 'Low'\n",
    "final_table.loc[final_table.sub_index>'1','Responsiveness to systematic changes'] = 'mid'\n",
    "final_table.loc[final_table.sub_index>'1.2','Responsiveness to systematic changes'] = 'High'\n",
    "final_table.loc[final_table.sub_index=='Insignificant','Responsiveness to systematic changes'] = 'Unclear/very low'\n",
    "final_table\n",
    "final_table.to_excel(\"/project/results/ols.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
