{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install qgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analysis and wrangling\n",
    "import researchpy as rp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "from itertools import chain\n",
    "# visualization\n",
    "from pandas.plotting import scatter_matrix\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from pprint import pprint\n",
    "%matplotlib inline\n",
    "#plotly\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "from plotly.offline import download_plotlyjs,init_notebook_mode, plot, iplot\n",
    "import plotly as py \n",
    "import plotly.graph_objs as go # plotly graphical object\n",
    "import plotly.io as pio\n",
    "pio.renderers.default='notebook'\n",
    "# setting the general visualization style\n",
    "sns.set_style('whitegrid')\n",
    "# feature engineering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Libraries for Statistical Models\n",
    "import statsmodels.api as sm\n",
    "# ignoring warnings in the notebook\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore') \n",
    "# To display full output \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# feature engineering\n",
    "import pandas_profiling as pp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# machine learning models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# model selection\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "\n",
    "# Tuning and Esembling\n",
    "from sklearn.model_selection import GridSearchCV        \n",
    "from sklearn.model_selection import RandomizedSearchCV \n",
    "from mlxtend.regressor import StackingCVRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_sku = pd.read_csv(\"/project/data_for_models/df_final.csv\",index_col=0)\n",
    "\n",
    "#df_sku['date'] = pd.to_datetime(df_sku['date'])\n",
    "#df_sub['date'] = pd.to_datetime(df_sub['date'])\n",
    "#df_mkt['date'] = pd.to_datetime(df_mkt['date'])\n",
    "df_sku.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking data classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subseg = df_sku.groupby(['Subsegment'], as_index=False)\n",
    "subseg = subseg.agg({\"full_name\": \"nunique\"})\n",
    "subseg\n",
    "brand = df_sku.groupby(['brand'], as_index=False)\n",
    "brand = brand.agg({\"full_name\": \"nunique\"})\n",
    "brand\n",
    "flavour = df_sku.groupby(['flavour'], as_index=False)\n",
    "flavour = flavour.agg({\"full_name\": \"nunique\"})\n",
    "flavour\n",
    "pack_type = df_sku.groupby(['pack_type'], as_index=False)\n",
    "pack_type = pack_type.agg({\"full_name\": \"nunique\"})\n",
    "pack_type\n",
    "weight = df_sku.groupby(['weight'], as_index=False)\n",
    "weight = weight.agg({\"full_name\": \"nunique\"})\n",
    "weight\n",
    "company = df_sku.groupby(['company'], as_index=False)\n",
    "company = company.agg({\"full_name\": \"nunique\"})\n",
    "company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## covid features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covid\n",
    "df_sku.loc[(df_sku.date < '2020-02-28'),'covid']='No Covid'\n",
    "df_sku.loc[(df_sku.date >= '2020-02-28')&(df_sku.date<'2020-03-26'),'covid']='prior to lockdown'\n",
    "df_sku.loc[(df_sku.date >= '2020-03-26')&(df_sku.date<'2020-05-10'),'covid']='1st lockdown'\n",
    "df_sku.loc[(df_sku.date >= '2020-05-10')&(df_sku.date<'2020-09-14'),'covid']='easing period'\n",
    "df_sku.loc[(df_sku.date >= '2020-09-14'),'covid']='2nd lockdown'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#winter: 15th January to 15th March\n",
    "df_sku.loc[(df_sku.date > '2019-01-15') & (df_sku.date <= '2019-03-15'),'seasons']='Winter'\n",
    "df_sku.loc[(df_sku.date > '2020-01-15') & (df_sku.date <= '2020-03-15'),'seasons']='Winter'\n",
    "df_sku.loc[(df_sku.date > '2021-01-15') & (df_sku.date <= '2021-03-15'),'seasons']='Winter'\n",
    "#spring: 15th March to 15th May\n",
    "df_sku.loc[(df_sku.date > '2019-03-15') & (df_sku.date <= '2019-05-15'),'seasons']='Spring'\n",
    "df_sku.loc[(df_sku.date > '2020-03-15') & (df_sku.date <= '2020-05-15'),'seasons']='Spring'\n",
    "df_sku.loc[(df_sku.date > '2021-03-15') & (df_sku.date <= '2021-05-15'),'seasons']='Spring'\n",
    "#Summer: 15th May to 15th August \n",
    "df_sku.loc[(df_sku.date > '2018-05-15') & (df_sku.date <= '2018-08-15'),'seasons']='Summer'\n",
    "df_sku.loc[(df_sku.date > '2019-05-15') & (df_sku.date <= '2019-08-15'),'seasons']='Summer'\n",
    "df_sku.loc[(df_sku.date > '2020-05-15') & (df_sku.date <= '2020-08-15'),'seasons']='Summer'\n",
    "# Autumn: 15th August to 15th December\n",
    "df_sku.loc[(df_sku.date > '2018-08-15') & (df_sku.date <= '2018-12-15'),'seasons']='Autumn'\n",
    "df_sku.loc[(df_sku.date > '2019-08-15') & (df_sku.date <= '2019-12-15'),'seasons']='Autumn'\n",
    "df_sku.loc[(df_sku.date > '2020-08-15') & (df_sku.date <= '2020-12-15'),'seasons']='Autumn'\n",
    "\n",
    "df_sku.loc[(df_sku.date > '2018-12-15') & (df_sku.date <= '2019-01-15'),'seasons']='christmas'\n",
    "df_sku.loc[(df_sku.date > '2019-12-15') & (df_sku.date <= '2020-01-15'),'seasons']='christmas'\n",
    "df_sku.loc[(df_sku.date > '2020-12-15') & (df_sku.date <= '2021-01-15'),'seasons']='christmas'\n",
    "df_sku.to_csv(\"/project/data_for_models/feature_eng1.csv\") \n",
    "df_sku.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mkt index & subsegment index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_sku = pd.read_csv(\"/project/data_for_models/feature_eng1.csv\",index_col=0)\n",
    "df_sub = pd.read_csv(\"/project/raw_data/Biscuit_subsegments.csv\",index_col=0)\n",
    "df_mkt = pd.read_csv(\"/project/raw_data/Sweet_Biscuits.csv\",index_col=0)\n",
    "\n",
    "df_mkt = df_mkt[['channel','date','Sales']]\n",
    "df_sub = df_sub[['channel','date','subsegments','Sales']]\n",
    "def create_retailer(df):\n",
    "    df.loc[(df.channel == 'Tesco'),'retailer']='Tesco'\n",
    "    df.loc[(df.channel == 'SSL'),'retailer']='Sainsbury'\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop(['channel'], axis=1,inplace=True)\n",
    "create_retailer(df_mkt)\n",
    "create_retailer(df_sub)\n",
    "df_mkt.rename(columns={'Sales':'mkt_index'}, inplace=True)\n",
    "df_sub.rename(columns={'Sales':'sub_index'}, inplace=True)\n",
    "df_sub.rename(columns={'subsegments':'Subsegment'}, inplace=True)\n",
    "df_sub.loc[(df_sub['Subsegment'] == 'HEALTHIER BISCUITS'),'Subsegment']='Healthier'\n",
    "df_sub.loc[(df_sub['Subsegment'] == 'CHOCOLATE BISCUIT BARS'),'Subsegment']='CBB'\n",
    "df_sub.loc[(df_sub['Subsegment'] == 'EVERYDAY TREATS'),'Subsegment']='EDT'\n",
    "df_sub.loc[(df_sub['Subsegment'] == 'EVERYDAY BISCUITS'),'Subsegment']='EDB'\n",
    "df_mkt\n",
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sku = pd.merge(df_sku, df_mkt, on=['retailer','date'], how='left')\n",
    "df_sku = pd.merge(df_sku, df_sub, on=['retailer','date','Subsegment'], how='left')\n",
    "df_sku.dropna(inplace=True)\n",
    "df_sku.describe()\n",
    "df_sku.to_csv(\"/project/data_for_models/feature_eng2.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "group = df_sku.groupby(['full_name'], as_index=False)\n",
    "group = group.agg({\"date\": \"nunique\"})\n",
    "group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sku = pd.read_csv(\"/project/data_for_models/feature_eng2.csv\",index_col=0)\n",
    "channel = ['Tesco Express','Tesco excl. Express','Sainsbury Local','Sainsbury excl Local']\n",
    "df_sku['Sales_p'] = df_sku['Sales']\n",
    "df_sku['mkt_index_p'] = df_sku['mkt_index']\n",
    "df_sku['sub_index_p'] = df_sku['sub_index']\n",
    "for name in group['full_name'].values.tolist():\n",
    "    for i in channel:\n",
    "        df_sku.loc[(df_sku.full_name == name)&\n",
    "                   (df_sku.channel == i),['Sales_p','mkt_index_p','sub_index_p']] = df_sku.loc[(df_sku.full_name == name)&\n",
    "                   (df_sku.channel == i),['Sales_p','mkt_index_p','sub_index_p']].pct_change(fill_method='ffill')\n",
    "df_sku.dropna(inplace=True)\n",
    "df_sku['Excess_sales_mkt'] = df_sku['Sales_p'] - df_sku['mkt_index_p']\n",
    "df_sku['Excess_sales_sub'] = df_sku['Sales_p'] - df_sku['sub_index_p']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def distribution(variable):\n",
    "    fig, ax = plt.subplots(figsize=(13,5));\n",
    "    df_sku[variable].hist(bins=40, ax=ax);\n",
    "    ax.set_title(\"Histogram of \" + variable, fontsize=16)\n",
    "    ax.set_xlabel(variable, fontsize=12);\n",
    "    ax.set_ylabel(\"Counts\", fontsize=12);\n",
    "distribution('Excess_sales_mkt')\n",
    "distribution('Excess_sales_sub')\n",
    "df_sku.describe()\n",
    "df_sku.to_csv(\"/project/data_for_models/feature_eng3.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependent variable for promo efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sku = pd.read_csv(\"/project/data_for_models/feature_eng3.csv\",index_col=0)\n",
    "#df_sku.loc[df.promo == 1,'Promo_Efficiency_index_sub'] = df['Excess_sales_sub']/(1+df['Promo_depth'])\n",
    "#df_sku.loc[df.promo == 0,'Promo_Efficiency_index_sub'] = 0\n",
    "#df_sku.loc[df.promo == 1,'Promo_Efficiency_index_mkt'] = df['Excess_sales_mkt']/(1+df['Promo_depth'])\n",
    "#df_sku.loc[df.promo == 0,'Promo_Efficiency_index_mkt'] = 0\n",
    "#df_sku.to_csv(\"/project/data_for_models/feature_eng3.csv\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot1 = df_sku.groupby(['channel'], as_index=False)[\"Promo_Efficiency_index_sub\"].mean()\n",
    "#plot1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew,probplot #for some statistics\n",
    "from scipy.special import boxcox1p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/project/data_for_models/feature_eng3.csv\",index_col=0)\n",
    "df = df.drop(['mkt_index','sub_index','Sales_p','mkt_index_p','sub_index_p'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weight'] = df['weight'].str.replace(r'GM$', '')\n",
    "df['weight'] = df['weight'].astype(str).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "numeric_feats = df.dtypes[df.dtypes == \"float\"].index\n",
    "\n",
    "# Check the skew of all numerical features\n",
    "skewed_feats = df[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "print(\"\\nSkew in numerical features: \\n\")\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "skewness.head(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def box_cox(example,lam_num):\n",
    "    fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(16, 16))\n",
    "    plt.subplots_adjust(top=1.5, right=1.5)\n",
    "    sns.distplot(df[example].dropna(), hist=True, ax=axs[0][0]);\n",
    "    probplot(df[example].dropna(), plot=axs[0][1]);\n",
    "    print('Original Sales to Market Skew: {}'.format(df[example].skew()));\n",
    "    print('Original Excess sales to Market Kurtosis: {}'.format(df[example].kurt()));\n",
    "    #df['Excess_sales_mkt'] = np.log1p(df['Excess_sales_mkt'])\n",
    "    #df['Excess_sales_sub'] = np.log1p(df['Excess_sales_sub'])\n",
    "    numeric_feats = df.dtypes[df.dtypes == \"float\"].index\n",
    "\n",
    "    # Check the skew of all numerical features\n",
    "    skewed_feats = df[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "    print(\"\\nSkew in numerical features: \\n\")\n",
    "    skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "    skewness = skewness[abs(skewness) > 0.75].dropna()\n",
    "    skewness = skewness.drop(['Excess_sales_sub','Excess_sales_mkt','distribution'])\n",
    "    print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
    "    skewed_features = skewness.index\n",
    "    for feat in skewed_features:\n",
    "        #df[feat] += 1\n",
    "        df[feat] = boxcox1p(df[feat], lam_num)\n",
    "\n",
    "    sns.distplot(df[example].dropna(), hist=True, ax=axs[1][0]);\n",
    "    probplot(df[example].dropna(), plot=axs[1][1]);\n",
    "\n",
    "    axs[0][0].set_xlabel(example +' to Market', size=20, labelpad=12.5)\n",
    "    axs[1][0].set_xlabel(example +' to Market', size=20, labelpad=12.5)\n",
    "    axs[0][1].set_xlabel('Theoretical Quantiles', size=20, labelpad=12.5)\n",
    "    axs[0][1].set_ylabel('Ordered Values', size=20)\n",
    "    axs[1][1].set_xlabel('Theoretical Quantiles', size=20, labelpad=12.5)\n",
    "    axs[1][1].set_ylabel('Ordered Values', size=20)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            axs[i][j].tick_params(axis='x', labelsize=15)\n",
    "            axs[i][j].tick_params(axis='y', labelsize=15)\n",
    "\n",
    "    axs[0][0].set_title(example +' of distribution', size=25, y=1.05)\n",
    "    axs[0][1].set_title(example +' Probability Plot', size=25, y=1.05)\n",
    "    axs[1][0].set_title('Distribution of '+ example + ' After box-cox Transformation', size=25, y=1.05)\n",
    "    axs[1][1].set_title(example +' Probability Plot After box-cox Transformation', size=25, y=1.05)\n",
    "    plt.show();\n",
    "    print('Box-Cox Promo_depth Skew: {}'.format(df[example].skew()));\n",
    "    print('Box-Cox Promo_depth Kurtosis: {}'.format(df[example].kurt()));\n",
    "box_cox('Sales',0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feats = df.dtypes[df.dtypes == \"float\"].index\n",
    "\n",
    "# Check the skew of all numerical features\n",
    "skewed_feats = df[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "print(\"\\nSkew in numerical features: \\n\")\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "skewness.head(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def scale(col_names,df):\n",
    "    # scaling numerical values\n",
    "    #Locate the attribute we need to standardize\n",
    "#    features = df[col_names]\n",
    "    # Use scaler of choice; here Standard scaler is used\n",
    "#    scaler = StandardScaler().fit(features.values)\n",
    "#    df[col_names] = scaler.transform(features.values)\n",
    "#col_name = ['Sales','distribution','price_per_unit','base_price1','Promo_depth']\n",
    "#for i in col_name:\n",
    "#    scale(col_name,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/project/data_for_models/feature_eng4.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# categorical encoding methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/project/data_for_models/feature_eng4.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding covid level of restrictions\n",
    "df.loc[(df.covid == 'No Covid'),'covid']=0\n",
    "df.loc[(df.covid == 'prior to lockdown'),'covid']=1\n",
    "df.loc[(df.covid == '1st lockdown'),'covid']=3\n",
    "df.loc[(df.covid == 'easing period'),'covid']=2\n",
    "df.loc[(df.covid == '2nd lockdown'),'covid']=3\n",
    "df['covid'] = df['covid'].astype(str).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding cyclic features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mnth_sin'] = np.sin((df.Month-1)*(2.*np.pi/12))\n",
    "df['mnth_cos'] = np.cos((df.Month-1)*(2.*np.pi/12))\n",
    "df['wk_sin'] = np.sin((df.WeekOfYear-1)*(2.*np.pi/52))\n",
    "df['wk_cos'] = np.cos((df.WeekOfYear-1)*(2.*np.pi/52))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['Month','WeekOfYear','price_per_kg'],axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/project/data_for_models/feature_eng5.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ignoring the curse of dimentionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cursed = pd.read_csv(\"/project/data_for_models/feature_eng5.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode seasons\n",
    "cat_var_to_enc = ['seasons','Subsegment','Year','channel']\n",
    "\n",
    "for var in cat_var_to_enc:\n",
    "    df = pd.concat([df,\\\n",
    "                          pd.get_dummies(df[var],\\\n",
    "                                         prefix=var, prefix_sep='_', drop_first=False)], axis=1)\n",
    "df = df.drop(['seasons','Subsegment','Year','retailer','format','channel'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode seasons\n",
    "cat_var_to_enc = ['flavour','pack_type']\n",
    "\n",
    "for var in cat_var_to_enc:\n",
    "    df_cursed = pd.concat([df_cursed,\\\n",
    "                          pd.get_dummies(df_cursed[var],\\\n",
    "                                         prefix=var, prefix_sep='_', drop_first=False)], axis=1)\n",
    "df_cursed = df_cursed.drop(['brand','flavour','pack_type'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode seasons\n",
    "cat_var_to_enc = ['full_name']\n",
    "\n",
    "for var in cat_var_to_enc:\n",
    "    df_cursed = pd.concat([df_cursed,\\\n",
    "                          pd.get_dummies(df_cursed[var],\\\n",
    "                                         prefix=var, prefix_sep='_', drop_first=False)], axis=1)\n",
    "df_cursed = df_cursed.drop(['full_name'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target1 = df_cursed['Excess_sales_mkt']\n",
    "Target2 = df_cursed['Excess_sales_sub']\n",
    "Target1.to_csv(\"/project/data_for_models/Target_mkt.csv\")\n",
    "Target2.to_csv(\"/project/data_for_models/Target_sub.csv\")\n",
    "df_cursed = df_cursed.drop(['Sales','company','date','Excess_sales_mkt','Excess_sales_sub'],axis=1)\n",
    "df_cursed.to_csv(\"/project/data_for_models/feature_eng_cursed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution to the curse of dimensionality 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grouping skus with similar consumer behaviours e.g., similar price elasiticity by clustering; thus skus can be packed into different clusters and hence the dimentions can be reduced through modeling on those clusters instead of the individual skus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price Elasticity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Price Elasticity gives the percentage change in quantity demanded in response to a one percent change in price. When you regress the log of volume against log of price, the coefficient gives you the slope i.e. change in volume w.r.t unit change in price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_solution1 = pd.read_csv(\"/project/data_for_models/feature_eng3.csv\",index_col=0)\n",
    "df_solution1['weight'] = df_solution1['weight'].str.replace(r'GM$', '')\n",
    "df_solution1['weight'] = df_solution1['weight'].astype(str).astype(float)\n",
    "df_solution1['units'] = df_solution1['Sales']/df_solution1['price_per_unit']\n",
    "df_solution1['units'] = np.log(df_solution1['units'])\n",
    "df_solution1['price_per_unit_log'] = np.log(df_solution1['price_per_unit'])\n",
    "\n",
    "channel1 = df_solution1['channel'].drop_duplicates().values.tolist()\n",
    "channel2 = ['Tesco Express', 'Tesco excl. Express']\n",
    "\n",
    "sku = df_solution1.groupby(['full_name'], as_index=False)['date'].count()\n",
    "channel_i = sku.loc[(sku.date >500)]['full_name'].values.tolist()\n",
    "channel_j = sku.loc[(sku.date <500)]['full_name'].values.tolist()\n",
    "\n",
    "skus = []\n",
    "channels = []\n",
    "price_elasticity = []\n",
    "r2 = []\n",
    "Avg_price = []\n",
    "Avg_sales = []\n",
    "weight = []\n",
    "Subsegment = []\n",
    "brand = []\n",
    "company = []\n",
    "Excess_sales_sub = []\n",
    "for i in channel_i:\n",
    "    for j in channel1:\n",
    "        reg = linear_model.LinearRegression()\n",
    "        x= df_solution1.loc[(df_solution1.full_name == i)&\n",
    "                            (df_solution1.channel== j),'units'].values.reshape(-1, 1)\n",
    "        y= df_solution1.loc[(df_solution1.full_name == i)&\n",
    "                            (df_solution1.channel== j),'price_per_unit_log'].values.reshape(-1, 1)\n",
    "        reg.fit(x,y)\n",
    "        r_sq = reg.score(x, y)\n",
    "        skus.append(df_solution1.loc[df_solution1['full_name'] == i]['full_name'].drop_duplicates().values.tolist())\n",
    "        channels.append(df_solution1.loc[df_solution1['channel'] == j]['channel'].drop_duplicates().values.tolist())\n",
    "        ceo = abs(reg.coef_)\n",
    "        price_elasticity.append(ceo)\n",
    "        r2.append(r_sq)\n",
    "        Avg_price.append(df_solution1.loc[(df_solution1.full_name == i)&\n",
    "                            (df_solution1.channel== j),'price_per_unit'].mean())\n",
    "        Avg_sales.append(df_solution1.loc[(df_solution1.full_name == i)&\n",
    "                            (df_solution1.channel== j),'Sales'].mean())\n",
    "        weight.append(df_solution1.loc[(df_solution1.full_name == i)&\n",
    "                            (df_solution1.channel== j),'weight'].drop_duplicates().values.tolist())\n",
    "        brand.append(df_solution1.loc[(df_solution1.full_name == i)&\n",
    "                            (df_solution1.channel== j),'brand'].drop_duplicates().values.tolist())\n",
    "        Subsegment.append(df_solution1.loc[(df_solution1.full_name == i)&\n",
    "                            (df_solution1.channel== j),'Subsegment'].drop_duplicates().values.tolist())\n",
    "        company.append(df_solution1.loc[(df_solution1.full_name == i)&\n",
    "                            (df_solution1.channel== j),'company'].drop_duplicates().values.tolist())\n",
    "        Excess_sales_sub.append(df_solution1.loc[(df_solution1.full_name == i)&\n",
    "                            (df_solution1.channel== j),'Excess_sales_sub'].mean())        \n",
    "for i in channel_j:\n",
    "    for j in channel2:\n",
    "        reg = linear_model.LinearRegression()\n",
    "        x= df_solution1.loc[(df_solution1.full_name == i)&\n",
    "                            (df_solution1.channel== j),'units'].values.reshape(-1, 1)\n",
    "        y= df_solution1.loc[(df_solution1.full_name == i)&\n",
    "                            (df_solution1.channel== j),'price_per_unit_log'].values.reshape(-1, 1)\n",
    "        reg.fit(x,y)\n",
    "        r_sq = reg.score(x, y)\n",
    "        skus.append(df_solution1.loc[df_solution1['full_name'] == i]['full_name'].drop_duplicates().values.tolist())\n",
    "        channels.append(df_solution1.loc[df_solution1['channel'] == j]['channel'].drop_duplicates().values.tolist())\n",
    "        ceo = abs(reg.coef_)\n",
    "        price_elasticity.append(ceo)\n",
    "        r2.append(r_sq)\n",
    "        Avg_price.append(df_solution1.loc[(df_solution1.full_name == i)&\n",
    "                            (df_solution1.channel== j),'price_per_unit'].mean())\n",
    "        Avg_sales.append(df_solution1.loc[(df_solution1.full_name == i)&\n",
    "                            (df_solution1.channel== j),'Sales'].mean())\n",
    "        weight.append(df_solution1.loc[(df_solution1.full_name == i)&\n",
    "                            (df_solution1.channel== j),'weight'].drop_duplicates().values.tolist())\n",
    "        brand.append(df_solution1.loc[(df_solution1.full_name == i)&\n",
    "                            (df_solution1.channel== j),'brand'].drop_duplicates().values.tolist())\n",
    "        Subsegment.append(df_solution1.loc[(df_solution1.full_name == i)&\n",
    "                            (df_solution1.channel== j),'Subsegment'].drop_duplicates().values.tolist())\n",
    "        company.append(df_solution1.loc[(df_solution1.full_name == i)&\n",
    "                            (df_solution1.channel== j),'company'].drop_duplicates().values.tolist())\n",
    "        Excess_sales_sub.append(df_solution1.loc[(df_solution1.full_name == i)&\n",
    "                            (df_solution1.channel== j),'Excess_sales_sub'].mean())   \n",
    "skus = list(chain.from_iterable(skus))\n",
    "channels = list(chain.from_iterable(channels))\n",
    "data = {'full_name':skus, \"channel\":channels,\"price_elasticity\": price_elasticity,'r2':r2,\"weight\": weight,\n",
    "       \"Avg_sales\": Avg_sales,\"Avg_price\": Avg_price,'Subsegment':Subsegment,'company':company,'brand':brand,'Excess_sales_sub':Excess_sales_sub}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elas = pd.DataFrame(data)\n",
    "#df_elas = df_flt.sort_values(by=['price_elasticity'], ascending=False)\n",
    "df_elas['price_elasticity'] = df_elas['price_elasticity'].str[0]\n",
    "df_elas['price_elasticity'] = df_elas['price_elasticity'].str[0]\n",
    "df_elas['weight'] = df_elas['weight'].str[0]\n",
    "df_elas['Subsegment'] = df_elas['Subsegment'].str[0]\n",
    "df_elas['company'] = df_elas['company'].str[0]\n",
    "df_elas['brand'] = df_elas['brand'].str[0]\n",
    "df_elas.to_csv(\"/project/data_for_models/elasticity.csv\")\n",
    "df_elas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_elas = pd.read_csv(\"/project/data_for_models/elasticity.csv\",index_col=0)\n",
    "df_elas = df_elas.drop(['full_name', 'r2'], axis=1)\n",
    "sns.pairplot(df_elas, hue='Subsegment', aspect=1.5)\n",
    "plt.show();\n",
    "df_elas = df_elas.drop([ \"channel\",'brand',], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elas.loc[(df_elas['Subsegment'] == 'Healthier'),'Subsegment']= 4\n",
    "df_elas.loc[(df_elas['Subsegment'] == 'CBB'),'Subsegment']= 3\n",
    "df_elas.loc[(df_elas['Subsegment'] == 'EDT'),'Subsegment']= 2 \n",
    "df_elas.loc[(df_elas['Subsegment'] == 'EDB'),'Subsegment']= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(data,x,y,n_clusters,random_state):\n",
    "    clusters = []\n",
    "    for i in range(1, 11):\n",
    "        km = KMeans(n_clusters=i,random_state=random_state).fit(data[[x,y]])\n",
    "        clusters.append(km.inertia_)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    sns.lineplot(x=list(range(1, 11)), y=clusters, ax=ax)\n",
    "    ax.set_title('Searching for Elbow')\n",
    "    ax.set_xlabel('Clusters')\n",
    "    ax.set_ylabel('Inertia')\n",
    "\n",
    "    plt.show();\n",
    "    \n",
    "    km3 = KMeans(n_clusters=n_clusters,random_state=random_state).fit(data[[x,y]])\n",
    "    data['Labels'] = km3.labels_\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.scatterplot(data[x], data[y], hue=data['Labels'], \n",
    "                    palette=sns.color_palette('hls', n_clusters))\n",
    "    plt.title('KMeans with '+ str(n_clusters) + ' Clusters')\n",
    "    plt.show();\n",
    "\n",
    "    fig = plt.figure(figsize=(20,8))\n",
    "    ax = fig.add_subplot(121)\n",
    "    sns.swarmplot(x='Labels', y=x, data=data, ax=ax)\n",
    "    ax.set_title('Labels According to '+x)\n",
    "\n",
    "    ax = fig.add_subplot(122)\n",
    "    sns.swarmplot(x='Labels', y=y, data=data, ax=ax)\n",
    "    ax.set_title('Labels According to '+y)\n",
    "\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clustering(df_elas,'price_elasticity','Excess_sales_sub',7,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bearing in mind that any price elasticity below 1 is considered as inelastic; thus the comparisons of price elasiticy below are based on their relative values. Higher price elasticity meaning consumers are more responsive to price changes, and promos are more likely to be effective, and vice versa.\n",
    "- Cluster 0 is mid price elasticity and high excess sales 'M-H'\n",
    "- Cluster 1 is mid price elasticity and low excess sales 'M-L'\n",
    "- Cluster 2 is very low price elasticity and low excess sales 'LL-L'\n",
    "- Cluster 3 is mid price elasticity and very high excess sales 'L-HH'\n",
    "- Cluster 4 is Low price elasticity and low excess sales 'L-L'\n",
    "- Cluster 5 is high price elasticity and low excess sales 'H-L'\n",
    "- Cluster 6 is mid price elasticity and mid excess sales 'M-M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elas_old = pd.read_csv(\"/project/data_for_models/elasticity.csv\",index_col=0)\n",
    "df_elas_new = df_elas_old.drop(['weight', 'Avg_sales','Avg_price','r2'], axis=1)\n",
    "df_elas_new = pd.merge(df_elas_new, df_elas[['Labels','Excess_sales_sub']],on=['Excess_sales_sub'])\n",
    "df_elas_new.rename(columns={'Labels':'clusters'}, inplace=True)\n",
    "#df_elas_new.to_csv(\"/project/data_for_models/clustered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elas_new.loc[(df_elas_new.clusters == 0),'clusters'] = 'M-H'\n",
    "df_elas_new.loc[(df_elas_new.clusters == 1),'clusters'] = 'M-L'\n",
    "df_elas_new.loc[(df_elas_new.clusters == 2),'clusters'] = 'LL-L'\n",
    "df_elas_new.loc[(df_elas_new.clusters == 3),'clusters'] = 'L-HH'\n",
    "df_elas_new.loc[(df_elas_new.clusters == 4),'clusters'] = 'L-L'\n",
    "df_elas_new.loc[(df_elas_new.clusters == 5),'clusters'] = 'H-L'\n",
    "df_elas_new.loc[(df_elas_new.clusters == 6),'clusters'] = 'M-M'\n",
    "#df_elas_new['clusters'] = df_elas_new['clusters'].astype(str).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "display(HTML(df_elas_new.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_elas_new.loc[(df_elas_new.company == 'PLADIS UK')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## applying the new clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/project/data_for_models/feature_eng5.csv\",index_col=0)\n",
    "df = pd.merge(df, df_elas_new[['full_name','channel','price_elasticity','clusters']], on=['full_name','channel'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode \n",
    "cat_var_to_enc = ['clusters','channel','flavour']\n",
    "\n",
    "for var in cat_var_to_enc:\n",
    "    df = pd.concat([df,\\\n",
    "                          pd.get_dummies(df[var],\\\n",
    "                                         prefix=var, prefix_sep='_', drop_first=False)], axis=1)\n",
    "df = df.drop(['clusters','channel','flavour'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode drop 1st\n",
    "cat_var_to_enc = ['retailer','format']\n",
    "\n",
    "for var in cat_var_to_enc:\n",
    "    df = pd.concat([df,\\\n",
    "                          pd.get_dummies(df[var],\\\n",
    "                                         prefix=var, prefix_sep='_', drop_first=True)], axis=1)\n",
    "df = df.drop(['retailer','format'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Subsegment_map = {'EDB': 0, 'EDT': 1, 'CBB': 2, 'Healthier': 3}\n",
    "df['Subsegment'] = df['Subsegment'].map(Subsegment_map)\n",
    "package_map = {'SINGLE': 1, '3 PACK': 3, '4 PACK': 4, '5 PACK': 5,'6 PACK':6,'8 PACK':8,'9 PACK':9}\n",
    "df['pack_type'] = df['pack_type'].map(package_map)\n",
    "df['Year'] = df['Year'].astype(str)\n",
    "year_map = {'2018': 1, '3 2019': 2, '2020': 3, '2021': 4}\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "season_map = {'christmas': 0, 'Winter': 1,'Spring': 2, 'Summer': 3, 'Autumn': 4}\n",
    "df['seasons'] = df['seasons'].map(season_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_less_cursed = df.drop(['full_name','Sales','company','date',\n",
    "                          'Excess_sales_mkt','Excess_sales_sub','brand','promo'],axis=1)\n",
    "df_less_cursed.to_csv(\"/project/data_for_models/feature_eng_less_cursed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_less_cursed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analytical = pd.read_csv(\"/project/data_for_models/feature_eng3.csv\",index_col=0)\n",
    "df_AD = (df_analytical.groupBy('key_one_above_model', \"key_model_level\",\"year_week\")\n",
    "        .agg(sum(\"tot_otd_price\").alias(\"tot_otd_price\"),\n",
    "             sum(\"tot_reg_price\").alias(\"tot_reg_price\"),\n",
    "            sum(\"tot_volume_sold\").alias(\"tot_volume_sold\")))                    \n",
    "unique_cat_for_correlation = (df_analytical.select(\"key_one_above_model\").distinct().collect())\n",
    "unique_cat_for_correlation = (list(map(lambda x: x['key_one_above_model'],unique_cat_for_correlation)))                                   \n",
    "df1 = df_AD.filter(df_AD['key_one_above_model'].isin (unique_cat_for_correlation))\n",
    "dummy1 = (df1.withColumnRenamed('key_model_level', 'Cannibals')\n",
    "          .select('key_one_above_model', 'Cannibals', 'year_week', 'discount_percent'))\n",
    "dummy2 = (df1\n",
    "          .select('key_one_above_model', 'key_model_level', 'year_week', 'tot_volume_sold'))\n",
    "df2 = (dummy2.join(dummy1, (dummy2.key_one_above_model == dummy1.key_one_above_model) &\n",
    "                   (dummy2.year_week == dummy1.year_week) &\n",
    "                   (dummy2.key_model_level != dummy1.Cannibals))\n",
    "       .drop(dummy1.key_one_above_model).drop(dummy1.year_week))\n",
    "count_df = (df2.groupBy('key_one_above_model', 'key_model_level', 'Cannibals').agg(F.count('year_week').alias('Weeks_sold_together')))\n",
    "keys = (count_df.select('key_model_level').rdd\n",
    "        .map(lambda x: x['key_model_level']).collect())\n",
    "cannibal_keys = (count_df.select('Cannibals').rdd\n",
    "                 .map(lambda x: x['Cannibals']).collect())\n",
    "product_combinations = list(zip(keys, cannibal_keys))\n",
    "for cat_id_volume, cat_id_discount in product_combinations:\n",
    "    print(cat_id_volume.split('_')[:-1] == cat_id_discount.split('_')[:-1],\n",
    "          cat_id_volume.split('_')[-1],\n",
    "          cat_id_discount.split('_')[-1])\n",
    "    df_corr = (df1.filter((df1[\"Cannibals\"] == cat_id_discount) &\n",
    "                        (df1[\"key_model_level\"] == cat_id_volume)))   \n",
    "    correlation_data.append({'cat_id_volume': cat_id_volume,\n",
    "                             'cat_id_discount': cat_id_discount,\n",
    "         'corr_value': df_corr.stat.corr('discount_percentage', 'tot_volume_sold')})\n",
    "corr_result = pd.DataFrame(correlation_data,columns = ['cat_id_volume','cat_id_discount','corr_value'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
