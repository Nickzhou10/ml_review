{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install xgboost\n",
    "!pip install linearmodels\n",
    "!pip install mlxtend\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "from itertools import chain\n",
    "# visualization\n",
    "from pandas.plotting import scatter_matrix\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from pprint import pprint\n",
    "import pandas_profiling as pp\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "#plotly\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "from plotly.offline import download_plotlyjs,init_notebook_mode, plot, iplot\n",
    "import plotly as py \n",
    "import plotly.graph_objs as go # plotly graphical object\n",
    "import plotly.io as pio\n",
    "pio.renderers.default='notebook'\n",
    "# setting the general visualization style\n",
    "sns.set_style('whitegrid')\n",
    "# feature engineering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Libraries for Statistical Models\n",
    "import statsmodels.api as sm\n",
    "# ignoring warnings in the notebook\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore') \n",
    "# To display full output \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# feature engineering\n",
    "import pandas_profiling as pp\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "# machine learning models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.impute import KNNImputer\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "# model selection\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, learning_curve\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "# Tuning and Esembling\n",
    "from sklearn.model_selection import GridSearchCV        \n",
    "from sklearn.model_selection import RandomizedSearchCV \n",
    "from mlxtend.regressor import StackingCVRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building plot for feature importance for each model\n",
    "def feature_importance(regressor, name,figsize1):\n",
    "# setting up the frame\n",
    "    fig, axes = plt.subplots(figsize = figsize1)\n",
    "# setting up parameters\n",
    "    indices = np.argsort(regressor.feature_importances_)[::-1][:30]\n",
    "# ploting feature importance\n",
    "    g = sns.barplot(y=X_train.columns[indices][:30],\n",
    "                x = regressor.feature_importances_[indices][:30],\n",
    "                orient='h')\n",
    "# labeling\n",
    "    g.set_xlabel(\"Relative importance\",fontsize=15)\n",
    "    g.set_ylabel(\"Features\",fontsize=15)\n",
    "    g.tick_params(labelsize=15)\n",
    "    g.set_title(name + \" feature importance\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def a function to plot the learning curves\n",
    "def plot_learning_curve(estimator, title, X, y, scoring, ylim = None, cv = None,\n",
    "                        n_jobs = -1, train_sizes = np.linspace(.1, 1.0, 5)):\n",
    "    # creating plt\n",
    "    plt.figure()\n",
    "    # labeling title\n",
    "    plt.title(title)\n",
    "    # setting y-axis limit if necessary\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    # labeling x,y axis\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    # calculating cross-validated training and test scores for different training set sizes.\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y,\n",
    "                                                            cv = cv, n_jobs = n_jobs, \n",
    "                                                            train_sizes = train_sizes,\n",
    "                                                            scoring = scoring, shuffle = True)\n",
    "    # calculating mean, stdv for scores\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    # plotting the above results and designing \n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "    # positioning legend\n",
    "    plt.legend(loc=\"best\")\n",
    "    # return the plot\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotly(name, mode):\n",
    "    if mode == 1:\n",
    "        fig = px.line(df_sku[df_sku['full_name']==name],\n",
    "                      x='date', y='Sales', title='Sales for '+ name,\n",
    "                      color='channel', template=\"none\")\n",
    "\n",
    "        fig.update_xaxes(\n",
    "            rangeslider_visible=True,\n",
    "            rangeselector=dict(\n",
    "                buttons=list([\n",
    "                    dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n",
    "                    dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n",
    "                    dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n",
    "                    dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n",
    "                    dict(step=\"all\")\n",
    "                ])\n",
    "            )\n",
    "        )\n",
    "        fig.show();\n",
    "        \n",
    "        fig = px.line(df_sku[df_sku['full_name']== name],\n",
    "                  x='date', y='units_sold', title='units_sold for '+ name,\n",
    "                  color='channel', template=\"none\")\n",
    "\n",
    "        fig.update_xaxes(\n",
    "            rangeslider_visible=True,\n",
    "            rangeselector=dict(\n",
    "                buttons=list([\n",
    "                    dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n",
    "                    dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n",
    "                    dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n",
    "                    dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n",
    "                    dict(step=\"all\")\n",
    "                ])\n",
    "            )\n",
    "        )\n",
    "        fig.show();\n",
    "        \n",
    "        fig = px.line(df_sku[df_sku['full_name']==name],\n",
    "                      x='date', y='kg_sold', title='kg_sold for '+ name,\n",
    "                      color='channel', template=\"none\")\n",
    "\n",
    "        fig.update_xaxes(\n",
    "            rangeslider_visible=True,\n",
    "            rangeselector=dict(\n",
    "                buttons=list([\n",
    "                    dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n",
    "                    dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n",
    "                    dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n",
    "                    dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n",
    "                    dict(step=\"all\")\n",
    "                ])\n",
    "            )\n",
    "        )\n",
    "        fig.show();\n",
    "\n",
    "        fig = px.line(df_sku[df_sku['full_name']==name],\n",
    "                      x='date', y='distribution', title='distribution for '+ name,\n",
    "                      color='channel', template=\"none\")\n",
    "\n",
    "        fig.update_xaxes(\n",
    "            rangeslider_visible=True,\n",
    "            rangeselector=dict(\n",
    "                buttons=list([\n",
    "                    dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n",
    "                    dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n",
    "                    dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n",
    "                    dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n",
    "                    dict(step=\"all\")\n",
    "                ])\n",
    "            )\n",
    "        )\n",
    "        fig.show();\n",
    "    else:\n",
    "        fig = px.line(df_sku[df_sku['full_name']== name],\n",
    "              x='date', y='price_per_unit', title='Price_per_units for '+ name,\n",
    "              color='channel', template=\"none\")\n",
    "\n",
    "        fig.update_xaxes(\n",
    "            rangeslider_visible=True,\n",
    "            rangeselector=dict(\n",
    "                buttons=list([\n",
    "                    dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n",
    "                    dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n",
    "                    dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n",
    "                    dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n",
    "                    dict(step=\"all\")\n",
    "                ])\n",
    "            )\n",
    "        )\n",
    "        fig.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sku_raw1 = pd.read_csv(\"/project/raw_data/SKU.csv\",index_col=0)\n",
    "df_sku_raw2 = pd.read_csv(\"/project/raw_data/SKU2.csv\",index_col=0)\n",
    "# rename rows\n",
    "df_sku_raw1.loc[df_sku_raw1['biscuit_category'] == 1, ['biscuit_category']] = 'EVERYDAY TREATS'\n",
    "df_sku_raw1.loc[df_sku_raw1['biscuit_category'] == 2, ['biscuit_category']] = 'EVERYDAY BISCUITS'\n",
    "df_sku_raw1.loc[df_sku_raw1['biscuit_category'] == 3, ['biscuit_category']] = 'CHOCOLATE BISCUIT BARS'\n",
    "# merge two dfs\n",
    "df_sku_raw = pd.merge(df_sku_raw1, df_sku_raw2, how=\"outer\")\n",
    "df_sku_raw = df_sku_raw.drop(columns=['sheet'])\n",
    "df_sku_raw = df_sku_raw.loc[(df_sku_raw.date < '2021-05-22')]\n",
    "df_sku_raw = df_sku_raw.loc[(df_sku_raw.date >= '2018-05-22')]\n",
    "df_raw = df_sku_raw\n",
    "# fix 0 and less than 0 data\n",
    "df_sku_raw[df_sku_raw['Sales'] <= 0]['Sales'] = 0\n",
    "df_sku_raw[df_sku_raw['units_sold'] <= 0]['units_sold'] = 0\n",
    "df_sku_raw[df_sku_raw['kg_sold'] <= 0]['kg_sold'] = 0\n",
    "# reshaping the levels\n",
    "df_sku_raw = df_sku_raw[df_sku_raw['channel'].isin(['Tesco Express','Tesco excl. Express','Sainsbury Local','Sainsbury excl Local'])]\n",
    "df_sku_raw.loc[(df_sku_raw.channel == 'Tesco Express'),'retailer']='Tesco'\n",
    "df_sku_raw.loc[(df_sku_raw.channel == 'Tesco excl. Express'),'retailer']='Tesco'\n",
    "df_sku_raw.loc[(df_sku_raw.channel == 'Sainsbury Local'),'retailer']='Sainsbury'\n",
    "df_sku_raw.loc[(df_sku_raw.channel == 'Sainsbury excl Local'),'retailer']='Sainsbury'\n",
    "# applying date dtype\n",
    "df_sku_raw['date'] = pd.to_datetime(df_sku_raw['date'])\n",
    "df_sku_raw['WeekOfYear'] = df_sku_raw.date.dt.weekofyear\n",
    "df_sku_raw['Year'] = df_sku_raw.date.dt.year\n",
    "df_sku_raw['Month'] = df_sku_raw.date.dt.month\n",
    "# renaming features\n",
    "df_sku_raw.loc[(df_sku_raw['biscuit_category'] == 'HEALTHIER BISCUITS'),'biscuit_category']='Healthier'\n",
    "df_sku_raw.loc[(df_sku_raw['biscuit_category'] == 'CHOCOLATE BISCUIT BARS'),'biscuit_category']='CBB'\n",
    "df_sku_raw.loc[(df_sku_raw['biscuit_category'] == 'EVERYDAY TREATS'),'biscuit_category']='EDT'\n",
    "df_sku_raw.loc[(df_sku_raw['biscuit_category'] == 'EVERYDAY BISCUITS'),'biscuit_category']='EDB'\n",
    "df_sku_raw.rename(columns={'biscuit_category':'Subsegment'}, inplace=True)\n",
    "# re-leveling\n",
    "df_sku_raw = df_sku_raw[~df_sku_raw['channel'].isin(['Tesco','Sainsbury'])]\n",
    "df_sku_raw.loc[(df_sku_raw.channel == 'Tesco Express'),'format']='Express'\n",
    "df_sku_raw.loc[(df_sku_raw.channel == 'Tesco excl. Express'),'format']='Main'\n",
    "df_sku_raw.loc[(df_sku_raw.channel == 'Sainsbury Local'),'format']='Express'\n",
    "df_sku_raw.loc[(df_sku_raw.channel == 'Sainsbury excl Local'),'format']='Main'\n",
    "# drop useless variabel\n",
    "df_sku_raw = df_sku_raw.drop(['off_shelf'], axis=1)\n",
    "\n",
    "# preview\n",
    "df_sku_raw\n",
    "df_sku_raw.info()\n",
    "df_sku_raw.describe()\n",
    "df_sku_raw.to_csv(\"/project/data_cleaning/SKU_new.csv\")  \n",
    "df_sku = df_sku_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_raw = df_raw.drop(['off_shelf'], axis=1)\n",
    "df_raw.profile_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def distribution(variable,title,x_title):\n",
    "    fig, ax = plt.subplots(figsize=(13,5));\n",
    "    df_sku_raw[variable].hist(bins=60, ax=ax);\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xlabel(x_title, fontsize=12);\n",
    "    ax.set_ylabel(\"Counts\", fontsize=12);\n",
    "#distribution('Sales')\n",
    "distribution('Sales','Figure 4: Histogram of Sales for SKU level dataset','Sales')\n",
    "distribution('distribution','Figure 5: Histogram of Distribution for SKU level dataset','Distribution')\n",
    "#distribution('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# identifying valid data points for each sku, based on the distribution(managing out of stock/delisting)\n",
    "Threshold = 0.2\n",
    "df_sku_gb = df_sku.groupby(['full_name'],as_index=False,sort=False).sum()\n",
    "Tesco_Express = []\n",
    "Tesco_Main = []\n",
    "Sainsbury_Local = []\n",
    "Sainsbury_Main = []\n",
    "product = df_sku['full_name'].drop_duplicates().values.tolist()\n",
    "company = []\n",
    "total_sales = df_sku_gb['Sales'].values.tolist()\n",
    "brand = []\n",
    "pack = []\n",
    "for i in product:\n",
    "    Tesco_Express.append(df_sku[(df_sku['full_name']== i) \n",
    "                                & (df_sku['distribution']>Threshold)\n",
    "                                & (df_sku['channel']=='Tesco Express')]['date'].count())\n",
    "    Tesco_Main.append(df_sku[(df_sku['full_name']== i) \n",
    "                                & (df_sku['distribution']>Threshold)\n",
    "                                & (df_sku['channel']=='Tesco excl. Express')]['date'].count())\n",
    "    Sainsbury_Local.append(df_sku[(df_sku['full_name']== i) \n",
    "                                & (df_sku['distribution']>Threshold)\n",
    "                                & (df_sku['channel']=='Sainsbury Local')]['date'].count())\n",
    "    Sainsbury_Main.append(df_sku[(df_sku['full_name']== i) \n",
    "                                & (df_sku['distribution']>Threshold)\n",
    "                                & (df_sku['channel']=='Sainsbury excl Local')]['date'].count())\n",
    "\n",
    "    company.append(df_sku.loc[df_sku['full_name'] == i]['company']\n",
    "                    .drop_duplicates().values.tolist())\n",
    "    brand.append(df_sku.loc[df_sku['full_name'] == i]['brand']\n",
    "                    .drop_duplicates().values.tolist())\n",
    "    pack.append(df_sku.loc[df_sku['full_name'] == i]['pack_type']\n",
    "                    .drop_duplicates().values.tolist())\n",
    "company = list(chain.from_iterable(company))\n",
    "brand = list(chain.from_iterable(brand))\n",
    "pack = list(chain.from_iterable(pack))\n",
    "data = {'full_name':product, \"Tesco_Express\":Tesco_Express,\"Tesco_Main\": Tesco_Main,\n",
    "        'Sainsbury_Local':Sainsbury_Local,'Sainsbury_Main':Sainsbury_Main,\n",
    "       'total_sales':total_sales,'company':company,'brand':brand, 'pack':pack}\n",
    "df_flt = pd.DataFrame(data)\n",
    "df_flt = df_flt.sort_values(by=['total_sales'], ascending=False)\n",
    "\n",
    "fuse1=150\n",
    "fuse2=0\n",
    "df_valid = df_flt[df_flt['Tesco_Express']>=fuse2]\n",
    "df_valid = df_valid[df_flt['Tesco_Main']>=fuse1]\n",
    "df_valid = df_valid[df_flt['Sainsbury_Local']>=fuse2]\n",
    "df_valid = df_valid[df_flt['Sainsbury_Main']>=fuse2]\n",
    "df_valid.reset_index(drop=True)\n",
    "#df_valid.to_csv(\"/project/temporary.csv\")  \n",
    "#df_valid.to_html(\"/project/data_cleaning/df_valid.html\") \n",
    "df_valid.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# select skus that has missing values\n",
    "df_problematic = df_valid[df_valid['Tesco_Express']!=156]\n",
    "# drop skus that cannot be imputated\n",
    "drop = [36,223,154,141,152,187,15,237,7,16]\n",
    "df_problematic = df_problematic.drop(index=drop)\n",
    "df_problematic\n",
    "#df_problematic.describe()\n",
    "df_not_bad = df_problematic.drop(df_problematic[(df_problematic['Sainsbury_Local']==0)].index)\n",
    "df_not_bad = df_not_bad.drop(df_not_bad[(df_not_bad['Sainsbury_Main']==0)].index)\n",
    "df_not_bad = df_not_bad.drop(index=37)\n",
    "df_not_bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB Imputing for large chunk of missing data for a single channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(col_names,df):\n",
    "    # scaling numerical values\n",
    "    #Locate the attribute we need to standardize\n",
    "    features = df[col_names]\n",
    "    # Use scaler of choice; here Standard scaler is used\n",
    "    scaler = StandardScaler().fit(features.values)\n",
    "    df[col_names] = scaler.transform(features.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sku.loc[(df_sku.distribution <= 0.2),'kg_sold']=0\n",
    "df_sku.loc[(df_sku.distribution <= 0.2),'units_sold']=0\n",
    "df_sku.loc[(df_sku.distribution <= 0.2),'distribution']=0\n",
    "df_sku.loc[(df_sku.distribution <= 0.2),'Sales']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute(full_name, variable,dependent, independ1,independ2, independ3, n_iter):\n",
    "    # select skus that has missing values\n",
    "    df_problematic = df_valid[df_valid['Tesco_Express']!=156]\n",
    "    # drop skus that cannot be imputated\n",
    "    drop = [36,223,154,141,152,187,15,237,7]\n",
    "    df_problematic = df_problematic.drop(index=drop)\n",
    "    #df_problematic\n",
    "    #df_problematic.describe()\n",
    "    df_not_bad = df_problematic.drop(df_problematic[(df_problematic['Sainsbury_Local']==0)].index)\n",
    "    df_not_bad = df_not_bad.drop(df_not_bad[(df_not_bad['Sainsbury_Main']==0)].index)\n",
    "    df_not_bad = df_not_bad.drop(index=37)\n",
    "    df_problematic = df_problematic.drop(columns=['total_sales','company','brand','pack'])\n",
    "    df_problematic=pd.melt(df_problematic,id_vars=['full_name'],var_name='channel', value_name='valid')\n",
    "    df_problematic = df_problematic.sort_values(by=['full_name'], ascending=False)\n",
    "    #df_problematic = df.drop(df[<some boolean condition>].index)\n",
    "    #df_problematic.to_csv(\"/project/data_cleaning/df_problematic.csv\")  \n",
    "    df_not_bad = df_sku[df_sku['full_name'].isin(df_not_bad['full_name'].values.tolist())]\n",
    "    # set sku\n",
    "    df_not_bad = df_not_bad[df_not_bad['full_name']==full_name]\n",
    "    # dropping useless ones\n",
    "    df_not_bad = df_not_bad.drop(columns=['format', 'flavour', 'brand', 'company', 'pack_type',\\\n",
    "                                   'weight', 'Subsegment','full_name','retailer'])\n",
    "    df_not_bad = df_not_bad.pivot_table(index=[\"date\",'WeekOfYear','Year','Month'], \n",
    "                        columns='channel', \n",
    "                        values=[variable]).reset_index()\n",
    "    df_not_bad.columns = ['_'.join(col) for col in df_not_bad.columns.values]\n",
    "    df_final = df_not_bad\n",
    "    # label encoding\n",
    "    #unit_encoder = LabelEncoder()\n",
    "    #df_not_bad['Year_'] = unit_encoder.fit_transform(df_not_bad['Year_'])\n",
    "    df_not_bad = df_not_bad.drop(columns=['date_'])\n",
    "    # data used to train and test model\n",
    "    data_model_pre = df_not_bad[~(df_not_bad[dependent]==0)]\n",
    "    data_model = data_model_pre\n",
    "    # data with missing gender and income info\n",
    "    data_predict = df_not_bad[df_not_bad[dependent]==0]\n",
    "    data_predict = data_predict.drop(columns=[dependent])\n",
    "    # scaling\n",
    "    col_names=[independ1,independ2, independ3]\n",
    "    if variable != 'distribution':\n",
    "        scale(col_names,data_predict)\n",
    "        scale(col_names,data_model)\n",
    "    #scaler = StandardScaler()\n",
    "    #data_model.loc[:,'units_sold_Sainsbury Local':'units_sold_Tesco excl. Express'] = scaler.fit_transform(data_model.loc[:,'units_sold_Sainsbury Local':'units_sold_Tesco excl. Express'])\n",
    "    #data_predict.loc[:,'units_sold_Sainsbury Local':'units_sold_Tesco excl. Express'] = scaler.fit_transform(data_predict.loc[:,'units_sold_Sainsbury Local':'units_sold_Tesco excl. Express'])\n",
    "    # train test split\n",
    "    train, test = train_test_split(data_model, test_size=0.2, random_state=42)\n",
    "    Y_train = data_model[dependent].values.ravel()\n",
    "    X_train = data_model.drop(columns=[dependent])\n",
    "    #Y_train = train['units_sold_Tesco Express'].values.ravel()\n",
    "    #X_train = train.drop(columns=['units_sold_Tesco Express'])\n",
    "    Y_test = test[dependent].values.ravel()\n",
    "    X_test = test.drop(columns=[dependent])\n",
    "    skf = KFold(n_splits=3, shuffle = True, random_state = 42)\n",
    "\n",
    "    params = {\n",
    "            'max_depth': [10, 20, 50,100],\n",
    "            'learning_rate': [0.01,0.05,0.1],\n",
    "            'subsample': [0.7,0.8,0.9],\n",
    "            'seed' :[42],\n",
    "            'colsample_bytree': [0.3, 0.5, 0.7],\n",
    "            'colsample_bynode': [0.3, 0.5, 0.7],\n",
    "            'colsample_bylevel': [0.3, 0.5, 0.7],\n",
    "            'min_child_weight': [5,7,10],\n",
    "            'n_estimators': [100, 200,500],\n",
    "            'gamma': [0.1, 0.25, 0.5],\n",
    "            'objective': ['reg:squarederror']}\n",
    "\n",
    "    model_unit = RandomizedSearchCV(estimator = XGBRegressor(), param_distributions=params,\n",
    "                             scoring='neg_root_mean_squared_error', n_jobs=4, n_iter = n_iter,\n",
    "                             cv=skf.split(X_train,Y_train),\n",
    "                             verbose=2)\n",
    "\n",
    "    model_unit.fit(X_train, Y_train)\n",
    "    modelcv = pd.DataFrame(model_unit.cv_results_)\n",
    "    modelcv\n",
    "    model_unit.best_params_\n",
    "    # building plot for feature importance for each model\n",
    "    def feature_importance(regressor, name,figsize1):\n",
    "    # setting up the frame\n",
    "        fig, axes = plt.subplots(figsize = figsize1)\n",
    "    # setting up parameters\n",
    "        indices = np.argsort(regressor.feature_importances_)[::-1][:30]\n",
    "    # ploting feature importance\n",
    "        g = sns.barplot(y=X_train.columns[indices][:30],\n",
    "                    x = regressor.feature_importances_[indices][:30],\n",
    "                    orient='h')\n",
    "    # labeling\n",
    "        g.set_xlabel(\"Relative importance\",fontsize=15)\n",
    "        g.set_ylabel(\"Features\",fontsize=15)\n",
    "        g.tick_params(labelsize=15)\n",
    "        g.set_title(name + \" feature importance\");\n",
    "    # plotting the feature importance for both of the best models\n",
    "    feature_importance(model_unit.best_estimator_, 'XGBoost Best Model ',(18,8))\n",
    "    g = plot_learning_curve(model_unit.best_estimator_,\n",
    "                            \"XGBoost best model learning curves: neg_mean_squared_error score\",\n",
    "                            X_train,Y_train, 'neg_root_mean_squared_error',ylim = [0, -50000],cv=3)\n",
    "    g = plot_learning_curve(model_unit.best_estimator_,\n",
    "                            \"XGBoost best model learning curves: neg_mean_absolute_error\",\n",
    "                            X_train,Y_train, 'neg_mean_absolute_error',ylim = [0, -50000],cv=3)\n",
    "    g = plot_learning_curve(model_unit.best_estimator_,\n",
    "                            \"XGBoost best model learning curves: r2\",\n",
    "                            X_train,Y_train, 'r2',ylim = [0, 1],cv=3)\n",
    "\n",
    "    #train_pred = model_unit.predict(X_train)\n",
    "    test_pred = model_unit.predict(X_test)\n",
    "\n",
    "    #train_mse = mean_squared_error(Y_train, train_pred)\n",
    "    #train_rmse = np.sqrt(train_mse)\n",
    "    #print('Training Set: Root Mean Squared Error (RMSE): '+ str(np.round(train_rmse,4)))\n",
    "    #rmspe_train = (np.sqrt(np.mean(np.square((Y_train - train_pred) / Y_train))))*100\n",
    "    #print('Training Set: Root Mean Squared percentage Error (RMSPE): '+ str(np.round(rmspe_train,4))+'%' +\n",
    "    #      ' (avg. accuracy: ' +str(100 - np.round(rmspe_train,4))+ '%)')\n",
    "    test_mse = mean_squared_error(Y_test, test_pred)\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "    print('Testing Set: Root Mean Squared Error (RMSE): '+ str(np.round(test_rmse,4)))\n",
    "    rmspe_test = (np.sqrt(np.mean(np.square((Y_test - test_pred) / Y_test))))*100\n",
    "    print('Testing Set: Root Mean Squared percentage Error (RMSPE): '+ str(np.round(rmspe_test,4))+'%' +\n",
    "          ' (avg. accuracy: ' +str(100 - np.round(rmspe_test,4))+ '%)')\n",
    "\n",
    "\n",
    "\n",
    "    df_final.loc[(df_final[dependent]==0),dependent] = model_unit.predict(data_predict)\n",
    "    df_final[dependent] = np.round(df_final[dependent],4)\n",
    "    df_sku.loc[(df_sku['full_name'] == full_name) & \n",
    "           (df_sku['channel'] == 'Tesco Express'),variable] = df_final[dependent].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute('GNGR MCVTS GNGR NTS GNGR 250 GM SNGL',\n",
    "       'Sales','Sales_Tesco Express',\n",
    "       'Sales_Sainsbury Local',\n",
    "      'Sales_Sainsbury excl Local',\n",
    "      'Sales_Tesco excl. Express',30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute('GNGR MCVTS GNGR NTS GNGR 250 GM SNGL',\n",
    "       'units_sold','units_sold_Tesco Express',\n",
    "       'units_sold_Sainsbury Local',\n",
    "      'units_sold_Sainsbury excl Local',\n",
    "      'units_sold_Tesco excl. Express',30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute('GNGR MCVTS GNGR NTS GNGR 250 GM SNGL',\n",
    "       'kg_sold','kg_sold_Tesco Express',\n",
    "       'kg_sold_Sainsbury Local',\n",
    "      'kg_sold_Sainsbury excl Local',\n",
    "      'kg_sold_Tesco excl. Express',30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute('GNGR MCVTS GNGR NTS GNGR 250 GM SNGL',\n",
    "       'distribution','distribution_Tesco Express',\n",
    "       'distribution_Sainsbury Local',\n",
    "      'distribution_Sainsbury excl Local',\n",
    "      'distribution_Tesco excl. Express',30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly('GNGR MCVTS GNGR NTS GNGR 250 GM SNGL',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "impute('JFF CKS DRK CHCLT & ORNG 244 GM SNGL',\n",
    "       'Sales','Sales_Tesco Express',\n",
    "       'Sales_Sainsbury Local',\n",
    "      'Sales_Sainsbury excl Local',\n",
    "      'Sales_Tesco excl. Express',30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute('JFF CKS DRK CHCLT & ORNG 244 GM SNGL',\n",
    "       'units_sold','units_sold_Tesco Express',\n",
    "       'units_sold_Sainsbury Local',\n",
    "      'units_sold_Sainsbury excl Local',\n",
    "      'units_sold_Tesco excl. Express',30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute('JFF CKS DRK CHCLT & ORNG 244 GM SNGL',\n",
    "       'kg_sold','kg_sold_Tesco Express',\n",
    "       'kg_sold_Sainsbury Local',\n",
    "      'kg_sold_Sainsbury excl Local',\n",
    "      'kg_sold_Tesco excl. Express',30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute('JFF CKS DRK CHCLT & ORNG 244 GM SNGL',\n",
    "       'distribution','distribution_Tesco Express',\n",
    "       'distribution_Sainsbury Local',\n",
    "      'distribution_Sainsbury excl Local',\n",
    "      'distribution_Tesco excl. Express',30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotly('JFF CKS DRK CHCLT & ORNG 244 GM SNGL',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sku.to_csv(\"/project/data_cleaning/imputed1.csv\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dropping missing data of sainsbury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sku = pd.read_csv(\"/project/data_cleaning/imputed1.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_name = ['CSTRD CRMS PRVT LBL CSTRD 400 GM SNGL',\n",
    "            'JFF CKS DRK CHCLT & ORNG 122 GM SNGL',\n",
    "            'STRWBRR&LV-YGRT 51 GM 5 PCK',\n",
    "            'TRTS JM FXS-JM\\'N-CRM RSPBRR 150 GM SNGL',\n",
    "            'BRS KLLGGS CC PPS BR CHCLT 20 GM 6 PCK',\n",
    "            'EVRD TRTS CKS PRVT LBL CHCLT 200 GM SNGL',\n",
    "            'BLVT-BRKFST-SFT-BKS BLBRR 50 GM 5 PCK']\n",
    "for i in drop_name:\n",
    "    df_sku.drop(df_sku.loc[(df_sku.full_name==i)&\n",
    "                           (df_sku.channel=='Sainsbury Local')].index, inplace=True)\n",
    "    df_sku.drop(df_sku.loc[(df_sku.full_name==i)&\n",
    "                           (df_sku.channel=='Sainsbury excl Local')].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors imputation for missing at random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotly('TWX FNGRS CRML & SHRTCK 23 GM 9 PCK',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotly('RC KRSPS SQRS MRSHMLLW 28 GM 4 PCK',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotly('BLVT-BRKFST-SFT-BKS CHC-CHPS 50 GM 5 PCK',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly('BRS KLLGGS CC PPS BR CHCLT 20 GM 6 PCK',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_impute(sku,neighbors,threshold):\n",
    "    df_sku.loc[(df_sku.full_name==sku)&\n",
    "               (df_sku.distribution==0),['units_sold','kg_sold','Sales']]=np.nan\n",
    "    df_sku.loc[(df_sku.full_name==sku)&\n",
    "               (df_sku.distribution==0),['distribution']]=threshold\n",
    "    imputer = KNNImputer(n_neighbors=neighbors,copy = False)\n",
    "    df_sku.loc[(df_sku.full_name==sku),\n",
    "               ['units_sold','kg_sold','Sales','distribution']] = imputer.fit_transform(df_sku.loc[(df_sku.full_name==sku),\n",
    "                                                                                                   ['units_sold','kg_sold','Sales','distribution']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_impute('TWX FNGRS CRML & SHRTCK 23 GM 9 PCK',3,0.22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_impute('RC KRSPS SQRS MRSHMLLW 28 GM 4 PCK',3,0.23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_impute('BLVT-BRKFST-SFT-BKS CHC-CHPS 50 GM 5 PCK',3,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_impute('BRS KLLGGS CC PPS BR CHCLT 20 GM 6 PCK',3,0.33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly('TWX FNGRS CRML & SHRTCK 23 GM 9 PCK',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotly('RC KRSPS SQRS MRSHMLLW 28 GM 4 PCK',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly('BLVT-BRKFST-SFT-BKS CHC-CHPS 50 GM 5 PCK',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly('BRS KLLGGS CC PPS BR CHCLT 20 GM 6 PCK',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dropping missing data at the beginning of the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly('TRTS OTHR CRM OR-THNS VNLL 192 GM SNGL',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_first(name,date):\n",
    "    df_sku.drop(df_sku.loc[(df_sku.full_name==name)&\n",
    "               (df_sku.date<=date)].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_first('TRTS OTHR CRM OR-THNS VNLL 192 GM SNGL','2018-07-14')\n",
    "drop_first('BLVT-BRKFST-SFT-BKS BLBRR 50 GM 5 PCK','2018-07-07')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotly('TRTS OTHR CRM OR-THNS VNLL 192 GM SNGL',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly('BLVT-BRKFST-SFT-BKS BLBRR 50 GM 5 PCK',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some final adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new variables\n",
    "df_sku['price_per_unit']=np.round(df_sku['Sales']/df_sku['units_sold'],3)\n",
    "df_sku['price_per_kg']=np.round(df_sku['Sales']/df_sku['kg_sold'],3)\n",
    "\n",
    "# fillna for new variables\n",
    "#df_sku['price_per_unit'] = df_sku['price_per_unit'].fillna(0.0)\n",
    "#df_sku['price_per_kg'] = df_sku['price_per_kg'].fillna(0.0)\n",
    "#df_sku.isnull().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sku.loc[(df_sku.full_name=='GNGR MCVTS GNGR NTS GNGR 250 GM SNGL')&\n",
    "           (df_sku.channel=='Tesco Express')&(df_sku.date<='2019-09-14'),'price_per_unit']=1\n",
    "df_sku.loc[(df_sku.full_name=='JFF CKS DRK CHCLT & ORNG 244 GM SNGL')&\n",
    "           (df_sku.channel=='Tesco Express')&(df_sku.date<='2019-09-14'),'price_per_unit']=df_sku.loc[\n",
    "           (df_sku.full_name=='JFF CKS DRK CHCLT & ORNG 244 GM SNGL')&\n",
    "           (df_sku.channel=='Tesco excl. Express')&(df_sku.date<='2019-09-14'),'price_per_unit'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sku.loc[(df_sku.full_name=='GNGR MCVTS GNGR NTS GNGR 250 GM SNGL')&\n",
    "           (df_sku.channel=='Tesco Express')&(df_sku.date<='2019-09-14'),\n",
    "           'units_sold']=df_sku.loc[(df_sku.full_name=='JFF CKS DRK CHCLT & ORNG 244 GM SNGL')&\n",
    "           (df_sku.channel=='Tesco Express')&(df_sku.date<='2019-09-14'),\n",
    "           'Sales']/df_sku.loc[(df_sku.full_name=='JFF CKS DRK CHCLT & ORNG 244 GM SNGL')&\n",
    "           (df_sku.channel=='Tesco Express')&(df_sku.date<='2019-09-14'),\n",
    "           'price_per_unit']\n",
    "df_sku.loc[(df_sku.full_name=='JFF CKS DRK CHCLT & ORNG 244 GM SNGL')&\n",
    "           (df_sku.channel=='Tesco Express')&(df_sku.date<='2019-09-14'),\n",
    "           'units_sold']=df_sku.loc[(df_sku.full_name=='JFF CKS DRK CHCLT & ORNG 244 GM SNGL')&\n",
    "           (df_sku.channel=='Tesco Express')&(df_sku.date<='2019-09-14'),\n",
    "           'Sales']/df_sku.loc[(df_sku.full_name=='JFF CKS DRK CHCLT & ORNG 244 GM SNGL')&\n",
    "           (df_sku.channel=='Tesco Express')&(df_sku.date<='2019-09-14'),\n",
    "           'price_per_unit']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sku.to_csv(\"/project/data_cleaning/imputed2.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_be_imputed = pd.read_csv(\"/project/data_cleaning/df_to_be_imputed.csv\",index_col=0)\n",
    "df_sku = pd.read_csv(\"/project/data_cleaning/imputed2.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_valid = pd.merge(df_valid, df_to_be_imputed, how=\"outer\")\n",
    "df_final_valid.to_csv(\"/project/data_cleaning/df_final_valid.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final1 = df_sku[df_sku['full_name'].isin(df_final_valid['full_name'].values.tolist())]\n",
    "df_final1.describe()\n",
    "df_final1.to_csv(\"/project/data_cleaning/df_final1.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# identifying valid data points for each sku, based on the distribution(managing out of stock/delisting)\n",
    "Threshold = 0.2\n",
    "df_sku_gb = df_sku.groupby(['full_name'],as_index=False,sort=False).sum()\n",
    "Tesco_Express = []\n",
    "Tesco_Main = []\n",
    "Sainsbury_Local = []\n",
    "Sainsbury_Main = []\n",
    "product = df_sku['full_name'].drop_duplicates().values.tolist()\n",
    "company = []\n",
    "total_sales = df_sku_gb['Sales'].values.tolist()\n",
    "brand = []\n",
    "pack = []\n",
    "for i in product:\n",
    "    Tesco_Express.append(df_sku[(df_sku['full_name']== i) \n",
    "                                & (df_sku['distribution']>Threshold)\n",
    "                                & (df_sku['channel']=='Tesco Express')]['date'].count())\n",
    "    Tesco_Main.append(df_sku[(df_sku['full_name']== i) \n",
    "                                & (df_sku['distribution']>Threshold)\n",
    "                                & (df_sku['channel']=='Tesco excl. Express')]['date'].count())\n",
    "    Sainsbury_Local.append(df_sku[(df_sku['full_name']== i) \n",
    "                                & (df_sku['distribution']>Threshold)\n",
    "                                & (df_sku['channel']=='Sainsbury Local')]['date'].count())\n",
    "    Sainsbury_Main.append(df_sku[(df_sku['full_name']== i) \n",
    "                                & (df_sku['distribution']>Threshold)\n",
    "                                & (df_sku['channel']=='Sainsbury excl Local')]['date'].count())\n",
    "\n",
    "    company.append(df_sku.loc[df_sku['full_name'] == i]['company']\n",
    "                    .drop_duplicates().values.tolist())\n",
    "    brand.append(df_sku.loc[df_sku['full_name'] == i]['brand']\n",
    "                    .drop_duplicates().values.tolist())\n",
    "    pack.append(df_sku.loc[df_sku['full_name'] == i]['pack_type']\n",
    "                    .drop_duplicates().values.tolist())\n",
    "company = list(chain.from_iterable(company))\n",
    "brand = list(chain.from_iterable(brand))\n",
    "pack = list(chain.from_iterable(pack))\n",
    "data = {'full_name':product, \"Tesco_Express\":Tesco_Express,\"Tesco_Main\": Tesco_Main,\n",
    "        'Sainsbury_Local':Sainsbury_Local,'Sainsbury_Main':Sainsbury_Main,\n",
    "       'total_sales':total_sales,'company':company,'brand':brand, 'pack':pack}\n",
    "df_flt = pd.DataFrame(data)\n",
    "df_flt = df_flt.sort_values(by=['total_sales'], ascending=False)\n",
    "\n",
    "fuse1=150\n",
    "fuse2=0\n",
    "df_valid = df_flt[df_flt['Tesco_Express']>=fuse1]\n",
    "df_valid = df_valid[df_flt['Tesco_Main']>=fuse1]\n",
    "df_valid = df_valid[df_flt['Sainsbury_Local']>=fuse2]\n",
    "df_valid = df_valid[df_flt['Sainsbury_Main']>=fuse2]\n",
    "df_valid\n",
    "#df_valid.to_csv(\"/project/data_cleaning/df_without_missing.csv\")  \n",
    "#df_valid.to_html(\"/project/data_cleaning/df_valid.html\") \n",
    "df_valid.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
